{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53cf98dd",
   "metadata": {},
   "source": [
    "Based on the full data subset A with no missing data, the full samples for y=0 and y=1 are learned and new samples are generated using the data and labels from tp2."
   ] 
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1ccfdb", 
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from skbio.stats.composition import clr, alr, ilr\n",
    "from scipy.stats import gaussian_kde\n",
    "import scipy.stats as stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94de462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_pred_proba):\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp)\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    return {\n",
    "#         'Accuracy': accuracy,\n",
    "        'Balanced_accuracy': balanced_accuracy,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "#         'F1 Score': f1,\n",
    "        'AUC': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2202c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb3d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTHONHASHSEED\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b804814",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b4ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-fold cross-validation\n",
    "# 1. There are positive samples in both the training and validation sets.\n",
    "# 2. the class distributions in the training and validation sets are similar to the original dataset.\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits= n_splits, shuffle=True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc040e25",
   "metadata": {},
   "source": [
    "### metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b094fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata = pd.read_csv(\"data/BASIC_metadata_full.csv\", sep=',', low_memory=False)\n",
    "Metadata = Metadata.drop(Metadata.columns[0], axis=1)\n",
    "\n",
    "# convert timepoits to 0,1,2\n",
    "Metadata.loc[Metadata.TimePoint == \"Trimester2\",\"TimePoint\"] = 0 \n",
    "Metadata.loc[Metadata.TimePoint == \"Trimester3\",\"TimePoint\"] = 1\n",
    "Metadata.loc[Metadata.TimePoint == \"PostpartumWeek6\",\"TimePoint\"] = 2\n",
    "\n",
    "# turn insufficient reads to NaN\n",
    "i = Metadata[Metadata.ReadsNumber < 500000].index\n",
    "Metadata.loc[i, 'ReadsNumber'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8dc56",
   "metadata": {},
   "source": [
    "### species data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f473d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile =pd.read_csv(\"data/Species_Profile_full.csv\",sep=',',low_memory=False)\n",
    "\n",
    "# extract all bacteria names\n",
    "full_list_bacteria = list(profile.columns)[1:]\n",
    "\n",
    "species = profile.to_numpy()[:,1:]\n",
    "\n",
    "species_num = np.shape(species)[1] # 713 species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9729239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join profile and metadeata\n",
    "merged_data_base = pd.merge(profile, Metadata, left_on='Sample_id', right_on='Sample_ID')\n",
    "\n",
    "merged_data = merged_data_base.dropna(subset=['ReadsNumber'])[['Individual_ID', 'TimePoint', 'EPDS', 'Dichotomous_EPDS'] + full_list_bacteria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09b0a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sample individuals whose EPDS != NaN at tp2\n",
    "individuals_with_na_epds_at_tp2 = merged_data[\n",
    "    (merged_data['TimePoint'] == 2) & (merged_data['EPDS'].isna())\n",
    "]['Individual_ID'].unique()\n",
    "\n",
    "data = merged_data[~merged_data['Individual_ID'].isin(individuals_with_na_epds_at_tp2)]\n",
    "\n",
    "# 2. Sample individuals with data at tp0, tp1 and tp2\n",
    "individuals_with_all_timepoints = data.groupby('Individual_ID').filter(lambda x: set(x['TimePoint']) >= {0, 1, 2})['Individual_ID'].unique()\n",
    "data = data[data['Individual_ID'].isin(individuals_with_all_timepoints)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c09730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that have a value of 0 at all time points for all samples\n",
    "columns_to_drop = []\n",
    "for col in full_list_bacteria:\n",
    "    if (data[col] == 0).all():\n",
    "        columns_to_drop.append(col)\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Update full_list_bacteria\n",
    "full_list_bacteria = [col for col in full_list_bacteria if col not in columns_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0e84f",
   "metadata": {},
   "source": [
    "### （1）Non-CLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c96676",
   "metadata": {},
   "source": [
    "### Comparison of Zero-value Replacement Methods\n",
    "\n",
    "#### Replacement with half of the non-zero minimum value of each row\n",
    "- **Effect description**: <span style=\"color:red;\"> The effect after feature extraction was poor compared to replacing with column, but significant after applying oversampling.<span>\n",
    "- **Applicable scenario**: Suitable for scenarios where the focus is on within-sample variation, such as comparing the relative abundances of different species within the same sample.\n",
    "- **Advantages**: Maintains the internal structure of the sample and reduces the impact of inter-sample variation.\n",
    "- **Disadvantages**: May lead to inconsistent replacement values across different samples, affecting cross-sample comparisons.\n",
    "\n",
    "---\n",
    "\n",
    "#### Replacement with half of the non-zero minimum value of each column\n",
    "- **Effect description**: <span style=\"color:red;\"> Oversampling fails to learn and generate new and better samples.<span>\n",
    "- **Applicable scenario**: Applicable when the focus is on the distribution of features (e.g., species) across different samples.\n",
    "- **Advantages**: Maintains the consistency of features across different samples, facilitating cross-sample comparisons.\n",
    "- **Disadvantages**: May ignore the internal structure of the sample, affecting the analysis of within-sample variation.\n",
    "\n",
    "---\n",
    "\n",
    "#### Replacement with default value 1e-10\n",
    "- **Effect description**: <span style=\"color:red;\"> The effect after feature extraction was poor compared to replacing with column, and also oversampling fails to learn and generate new and better samples.<span>\n",
    "- **Applicable scenario**: Suitable when zero values result from measurement limitations and the replacement value minimally impacts overall analysis, e.g., in preliminary data exploration.\n",
    "- **Advantages**: imple to implement, no complex calculations or parameter estimations needed. Quickly handles zero values for further processing.\n",
    "- **Disadvantages**: Ignores data characteristics like non - zero value distributions. Fixed replacement may not reflect true zero - valued data, affecting analysis accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "374f4ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Replace the zero value by 1/2 of the non - zero minimum value\n",
    "data[full_list_bacteria] = data[full_list_bacteria].astype(float)\n",
    "\n",
    "# Replace with 1/2 of the non-zero minimum value of the row\n",
    "matrix = data[full_list_bacteria].values\n",
    "for i in range(matrix.shape[0]):\n",
    "    row = matrix[i, :]\n",
    "    non_zero_values = row[~np.isnan(row) & (row > 0)]\n",
    "    if len(non_zero_values) > 0:\n",
    "        min_non_zero = np.min(non_zero_values)\n",
    "        half_min = min_non_zero / 2\n",
    "        row[row == 0] = half_min\n",
    "    matrix[i, :] = row\n",
    "\n",
    "data[full_list_bacteria] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d86d36f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用 pandas 求和结果: 100.01409000000001\n"
     ]
    }
   ],
   "source": [
    "sum_first_row_pandas = data[full_list_bacteria].iloc[0].sum()\n",
    "print(\"使用 pandas 求和结果:\", sum_first_row_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6a3a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "for i in range(matrix.shape[0]):\n",
    "    row = matrix[i, :]\n",
    "    \n",
    "    row_sum = np.sum(row)\n",
    "    row = row / row_sum\n",
    "    matrix[i, :] = row\n",
    "\n",
    "data[full_list_bacteria] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53d65d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data from [individual * tp, feature] into [individual, tp, feature]\n",
    "grouped = data.groupby('Individual_ID')\n",
    "\n",
    "transformed_data = []\n",
    "labels = []\n",
    "\n",
    "for individual_id, group in grouped:\n",
    "    time_point_matrix = np.full((3, len(full_list_bacteria)), np.nan)\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        time_point = int(row['TimePoint'])\n",
    "        time_point_matrix[time_point] = row[full_list_bacteria].values\n",
    "\n",
    "    tp2_row = group[group['TimePoint'] == 2]\n",
    "    label = tp2_row['Dichotomous_EPDS'].values[0]\n",
    "\n",
    "    transformed_data.append(time_point_matrix)\n",
    "    labels.append(label)\n",
    "\n",
    "transformed_data = np.array(transformed_data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "735a337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority class： 15\n",
      "majority class： 70\n"
     ]
    }
   ],
   "source": [
    "print(\"minority class：\", len(labels[labels == 1]))\n",
    "print(\"majority class：\", len(labels[labels == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49063b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "  Balanced_accuracy: 0.5000 ± 0.0000\n",
      "  Sensitivity: 0.0000 ± 0.0000\n",
      "  Specificity: 1.0000 ± 0.0000\n",
      "  AUC: 0.5643 ± 0.1945\n"
     ]
    }
   ],
   "source": [
    "# Build an RF model on the data to make predictions with only CLR.\n",
    "# There are a lot of NAs in features\n",
    "X = transformed_data[:, :2, :].reshape(transformed_data.shape[0], -1)\n",
    "y = labels\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf.predict(X_val)\n",
    "    y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9073f",
   "metadata": {},
   "source": [
    "### （2）Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c097a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values in Dichotomous_EPDS at tp0, tp1 of data\n",
    "extracted_data_base = data.dropna(subset=['Dichotomous_EPDS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c65f5859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features for TimePoint 0: Index(['Allisonella_histaminiformans', 'Lactococcus_lactis',\n",
      "       'Enterococcus_faecium'],\n",
      "      dtype='object')\n",
      "Selected features for TimePoint 1: Index(['Enterococcus_faecalis', 'Allisonella_histaminiformans',\n",
      "       'Roseburia_sp_CAG_303'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for time_point in [0, 1]:\n",
    "    time_point_data = extracted_data_base[extracted_data_base['TimePoint'] == time_point]\n",
    "    \n",
    "    X = time_point_data.drop(['Individual_ID', 'TimePoint', 'EPDS', 'Dichotomous_EPDS'], axis=1)\n",
    "    y = time_point_data['Dichotomous_EPDS']\n",
    "\n",
    "    selector = SelectKBest(score_func=f_classif, k = 3)  # Select the k most important features\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "\n",
    "    feature_indices = selector.get_support(indices=True)\n",
    "    feature_columns = X.columns[feature_indices]\n",
    "    \n",
    "    print(f\"Selected features for TimePoint {time_point}: {feature_columns}\")\n",
    "\n",
    "    if time_point == 0:\n",
    "        selected_features_tp0 = feature_columns\n",
    "    elif time_point == 1:\n",
    "        selected_features_tp1 = feature_columns\n",
    "        \n",
    "feature_columns = list(set(selected_features_tp0) | set(selected_features_tp1))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6123c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data after feature selection\n",
    "unused_bacteria_columns = [col for col in full_list_bacteria if col not in feature_columns]\n",
    "others_column = data[unused_bacteria_columns].sum(axis=1).to_frame(name='Others')\n",
    "\n",
    "data = data[['Individual_ID', 'TimePoint', 'EPDS', 'Dichotomous_EPDS'] + list(feature_columns)]\n",
    "data = pd.concat([data, others_column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2e8ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLR\n",
    "columns_to_transform = [col for col in data.columns if col not in ['Individual_ID', 'TimePoint', 'EPDS', 'Dichotomous_EPDS']]\n",
    "transformed_data = data[columns_to_transform]\n",
    "\n",
    "clr_transformed = clr(transformed_data.values)\n",
    "clr_transformed_df = pd.DataFrame(clr_transformed, columns=columns_to_transform, index=transformed_data.index)\n",
    "\n",
    "data.loc[:, columns_to_transform] = clr_transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f96dfa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data from [individual * tp, feature] into [individual, tp, feature]\n",
    "grouped = data.groupby('Individual_ID')\n",
    "\n",
    "extracted_data = []\n",
    "labels = []\n",
    "\n",
    "for individual_id, group in grouped:\n",
    "    time_point_matrix = np.full((3, len(feature_columns)), np.nan)\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        time_point = int(row['TimePoint'])\n",
    "        time_point_matrix[time_point] = row[feature_columns].values\n",
    "\n",
    "    tp2_row = group[group['TimePoint'] == 2]\n",
    "    label = tp2_row['Dichotomous_EPDS'].values[0]\n",
    "\n",
    "    extracted_data.append(time_point_matrix)\n",
    "    labels.append(label)\n",
    "\n",
    "extracted_data = np.array(extracted_data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af97f6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority class： 15\n",
      "majority class： 70\n"
     ]
    }
   ],
   "source": [
    "print(\"minority class：\", len(labels[labels == 1]))\n",
    "print(\"majority class：\", len(labels[labels == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "831786e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "  Balanced_accuracy: 0.5643 ± 0.2038\n",
      "  Sensitivity: 0.2000 ± 0.4000\n",
      "  Specificity: 0.9286 ± 0.0782\n",
      "  AUC: 0.6048 ± 0.2007\n"
     ]
    }
   ],
   "source": [
    "# Only feature selection, with a small increase in sensitivity\n",
    "X = extracted_data[:, :2, :].reshape(extracted_data.shape[0], -1)\n",
    "y = labels\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf.predict(X_val)\n",
    "    y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32012d48",
   "metadata": {},
   "source": [
    "### （3）Feature Extraction + OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d78f0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data segmentation\n",
    "# Use data containing all time points when oversampling\n",
    "X_all = extracted_data[:, :, :].reshape(extracted_data.shape[0], -1)\n",
    "# Only data from tp0 and tp1 are used for modeling.\n",
    "X = extracted_data[:, :2, :].reshape(extracted_data.shape[0], -1)\n",
    "y = labels\n",
    "\n",
    "# Assuming the number of new samples generated\n",
    "num_new_samples = 55 # 70-15 = 55\n",
    "\n",
    "# Assume here that the minority class samples are originally in the shape (number of samples, 2, number of features), \n",
    "# and take this into account when recovering the shape\n",
    "original_num_time_steps = 3\n",
    "original_num_features = extracted_data.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bced50",
   "metadata": {},
   "source": [
    "### cGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88609d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/500] [D loss: 0.5122999548912048] [G loss: 0.6745175123214722]\n",
      "[Epoch 0/500] [D loss: 1.0727481821959373e-05] [G loss: 10.903702735900879]\n",
      "[Epoch 0/500] [D loss: 2.2065196390030906e-06] [G loss: 12.780921936035156]\n",
      "[Epoch 0/500] [D loss: 5.421073865363724e-07] [G loss: 14.312573432922363]\n",
      "[Epoch 0/500] [D loss: 7.58870370987097e-08] [G loss: 15.701498031616211]\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.6310 ± 0.1774\n",
      "  Sensitivity: 0.3333 ± 0.3651\n",
      "  Specificity: 0.9286 ± 0.0452\n",
      "  AUC: 0.6548 ± 0.1737\n"
     ]
    }
   ],
   "source": [
    "# Generate corresponding time point information for each sample\n",
    "num_samples = X_all.shape[0]\n",
    "\n",
    "time_points = np.tile(np.arange(original_num_time_steps), num_samples)\n",
    "# Encoded for each time point\n",
    "time_point_encoded = pd.get_dummies(time_points).values.reshape(num_samples, original_num_time_steps,\n",
    "                                                                original_num_time_steps)\n",
    "time_point_encoded_tensor = torch.tensor(time_point_encoded, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_shape, time_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.output_shape = output_shape\n",
    "        self.time_dim = time_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + time_dim * output_shape[0], 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(128, momentum=0.8),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(256, momentum=0.8),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(512, momentum=0.8),\n",
    "            nn.Linear(512, np.prod(output_shape)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, t):\n",
    "        t = t.reshape(-1, self.time_dim * self.output_shape[0])\n",
    "        zt = torch.cat([z, t], dim=1)\n",
    "        output = self.model(zt)\n",
    "        output = output.view(output.size(0), *self.output_shape)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape, time_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_shape = input_shape \n",
    "        self.time_dim = time_dim\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(np.prod(input_shape) + time_dim * input_shape[0], 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = x.reshape(-1, np.prod(self.input_shape))\n",
    "        t = t.reshape(-1, self.time_dim * self.input_shape[0])\n",
    "        xt = torch.cat([x, t], dim=1)\n",
    "        return self.model(xt)\n",
    "\n",
    "\n",
    "class tsGAN:\n",
    "    def __init__(self, latent_dim, output_shape, lr=0.0002, b1=0.5, b2=0.999):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_shape = output_shape\n",
    "        self.time_dim = output_shape[0]\n",
    "\n",
    "        self.generator = Generator(latent_dim, output_shape, self.time_dim)\n",
    "        self.discriminator = Discriminator(output_shape, self.time_dim)\n",
    "\n",
    "        self.optimizer_G = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        self.optimizer_D = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.generator.to(self.device)\n",
    "        self.discriminator.to(self.device)\n",
    "\n",
    "    def train(self, X_train, epochs, batch_size):\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32).to(self.device)\n",
    "        time_points_train = time_point_encoded_tensor[torch.tensor(np.arange(len(X_train)))]\n",
    "        time_points_train = time_points_train.to(self.device)\n",
    "\n",
    "        dataset = TensorDataset(X_train, time_points_train)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.discriminator.train()\n",
    "            self.generator.eval()\n",
    "\n",
    "            for real_data, real_time in dataloader:\n",
    "                real_labels = torch.ones(real_data.size(0), 1).to(self.device)\n",
    "                noise = torch.randn(real_data.size(0), self.latent_dim).to(self.device)\n",
    "                fake_data = self.generator(noise, real_time)\n",
    "                fake_labels = torch.zeros(real_data.size(0), 1).to(self.device)\n",
    "\n",
    "                # Calculation of discriminator loss\n",
    "                self.optimizer_D.zero_grad()\n",
    "                real_loss = self.loss_fn(self.discriminator(real_data, real_time), real_labels)\n",
    "                fake_loss = self.loss_fn(self.discriminator(fake_data.detach(), real_time), fake_labels)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "                d_loss.backward()\n",
    "                self.optimizer_D.step()\n",
    "\n",
    "            self.discriminator.eval()\n",
    "            self.generator.train()\n",
    "\n",
    "            for real_data, real_time in dataloader:\n",
    "                noise = torch.randn(real_data.size(0), self.latent_dim).to(self.device)\n",
    "                fake_data = self.generator(noise, real_time)\n",
    "                valid_labels = torch.ones(real_data.size(0), 1).to(self.device)\n",
    "\n",
    "                # Calculate the generator loss\n",
    "                self.optimizer_G.zero_grad()\n",
    "                g_loss = self.loss_fn(self.discriminator(fake_data, real_time), valid_labels)\n",
    "                g_loss.backward()\n",
    "                self.optimizer_G.step()\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"[Epoch {epoch}/{epochs}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "\n",
    "    def generate_samples(self, num_samples):\n",
    "        self.generator.eval()\n",
    "        new_time_points = np.tile(np.arange(self.time_dim), num_samples)\n",
    "        new_time_points_encoded = pd.get_dummies(new_time_points).values.reshape(num_samples, self.time_dim,\n",
    "                                                                                self.time_dim)\n",
    "        new_time_points_encoded_tensor = torch.tensor(new_time_points_encoded, dtype=torch.float32).to(self.device)\n",
    "        noise = torch.randn(num_samples, self.latent_dim).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            fake_data = self.generator(noise, new_time_points_encoded_tensor)\n",
    "        return fake_data.cpu().numpy()\n",
    "\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "# Initialize tsGAN\n",
    "latent_dim = 100\n",
    "output_shape = (original_num_time_steps, original_num_features)\n",
    "tsgan = tsGAN(latent_dim, output_shape)\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_all, y)):\n",
    "    X_train_all, X_val_all = X_all[train_idx], X_all[val_idx]\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Train tsGAN with all data (both y=0 and y=1)\n",
    "    tsgan.train(X_train_all, epochs=500, batch_size=16)\n",
    "\n",
    "    # Generate new samples using only data with y=1\n",
    "    minority_class = 1\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    minority_X_all = X_train_all[minority_indices]\n",
    "\n",
    "    # Generate new samples\n",
    "    new_samples_np = tsgan.generate_samples(num_new_samples)\n",
    "    new_samples_np = new_samples_np.reshape(num_new_samples, -1)\n",
    "\n",
    "    # Merge\n",
    "    combined_X_train_all = np.concatenate([X_train_all, new_samples_np], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, np.full(num_new_samples, minority_class)], axis=0)\n",
    "\n",
    "    combined_X_train = combined_X_train_all[:, :2 * original_num_features]\n",
    "\n",
    "    # Training a Random Forest Classifier\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n",
    "    rf.fit(combined_X_train, combined_y_train)\n",
    "\n",
    "    # Evaluating models on validation sets\n",
    "    y_pred = rf.predict(X_val)\n",
    "    y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c35ea",
   "metadata": {},
   "source": [
    "### GMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701389eb",
   "metadata": {},
   "source": [
    "The code does not consider time dependency.\n",
    "\n",
    "Modeling by time points: GMM modeling is performed on the data at different time points separately, and then when generating new samples, the corresponding models are selected to be sampled according to the time points, and then combined to form a complete sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5c29178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "  Balanced_accuracy: 0.5905 ± 0.1789\n",
      "  Sensitivity: 0.4667 ± 0.4000\n",
      "  Specificity: 0.7143 ± 0.0782\n",
      "  AUC: 0.6310 ± 0.1758\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_all, y)):\n",
    "    X_train_all, X_val_all = X_all[train_idx], X_all[val_idx]\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "    gmm.fit(X_train_all)  \n",
    "\n",
    "    minority_class = 1\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "\n",
    "    new_samples_flattened, _ = gmm.sample(num_new_samples)\n",
    "\n",
    "    new_samples_np = new_samples_flattened.reshape(num_new_samples, original_num_time_steps, original_num_features)\n",
    "    new_samples_np = new_samples_np.reshape(num_new_samples, -1)  # To test the validity of a new sample.\n",
    "\n",
    "    combined_X_train_all = np.concatenate([X_train_all, new_samples_np], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, np.full(num_new_samples, minority_class)], axis=0)\n",
    "\n",
    "    combined_X_train = combined_X_train_all[:, :2 * original_num_features]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n",
    "    rf.fit(combined_X_train, combined_y_train)\n",
    "\n",
    "    y_pred = rf.predict(X_val)\n",
    "    y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d742aaa",
   "metadata": {},
   "source": [
    "### CVAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da1771",
   "metadata": {},
   "source": [
    "For each sample, the time point information is uniquely encoded (pd.get_dummies) to obtain time_point_encoded and converted to the PyTorch tensor time_point_encoded_tensor.\n",
    "\n",
    "The discrete-time parameters are added to the CVAE model by splicing the time-point encoded information with the input data and category information (torch.cat) as input to the model during the encode and decode process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c119a7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "  Balanced_accuracy: 0.6167 ± 0.1797\n",
      "  Sensitivity: 0.3333 ± 0.3651\n",
      "  Specificity: 0.9000 ± 0.0728\n",
      "  AUC: 0.6595 ± 0.1720\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, cond_dim, time_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim * original_num_time_steps + cond_dim + time_dim * original_num_time_steps, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + cond_dim + time_dim * original_num_time_steps, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_dim * original_num_time_steps),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c, t):\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        t = t.reshape(t.size(0), -1)\n",
    "        xct = torch.cat([x, c, t], dim=1)\n",
    "        h = self.encoder(xct)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c, t):\n",
    "        t = t.reshape(t.size(0), -1)\n",
    "        zct = torch.cat([z, c, t], dim=1)\n",
    "        recon_x = self.decoder(zct)\n",
    "        recon_x = recon_x.reshape(recon_x.size(0), original_num_time_steps, -1)\n",
    "        return recon_x\n",
    "\n",
    "    def forward(self, x, c, t):\n",
    "        mu, logvar = self.encode(x, c, t)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z, c, t)\n",
    "        return recon_x, mu, logvar\n",
    "\n",
    "\n",
    "# hyperparameterization based on grid search\n",
    "input_dim = original_num_features\n",
    "latent_dim = 5\n",
    "cond_dim = len(np.unique(y))\n",
    "time_dim = original_num_time_steps\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_all, y)):\n",
    "    X_train_all, X_val_all = X_all[train_idx], X_all[val_idx]\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    all_X = X_train_all.reshape(-1, original_num_time_steps, original_num_features)\n",
    "    all_y = y_train\n",
    "\n",
    "    time_points = np.tile(np.arange(original_num_time_steps), len(all_X))\n",
    "    time_point_encoded = pd.get_dummies(time_points).values.reshape(-1, original_num_time_steps, original_num_time_steps)\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    all_X_tensor = torch.tensor(all_X, dtype=torch.float32)\n",
    "    all_y_tensor = torch.tensor(all_y, dtype=torch.long)\n",
    "    time_point_encoded_tensor = torch.tensor(time_point_encoded, dtype=torch.float32)\n",
    "\n",
    "    # Creating a DataLoader\n",
    "    dataset = TensorDataset(all_X_tensor, all_y_tensor, time_point_encoded_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Initializing the model, optimizer, and loss function\n",
    "    model = CVAE(input_dim, latent_dim, cond_dim, time_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_X, batch_y, batch_t in dataloader:\n",
    "            batch_c = torch.nn.functional.one_hot(batch_y, num_classes=cond_dim).float()\n",
    "            optimizer.zero_grad()\n",
    "            recon_x, mu, logvar = model(batch_X, batch_c, batch_t)\n",
    "\n",
    "            recon_loss = criterion(recon_x, batch_X)\n",
    "            kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "            loss = recon_loss + kl_divergence\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # print(f'Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "    # Generate new samples using only data with y=1\n",
    "    minority_class = 1\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    minority_X = X_train_all[minority_indices].reshape(-1, original_num_time_steps, original_num_features)\n",
    "    minority_y = y_train[minority_indices]\n",
    "\n",
    "    # Generate new samples\n",
    "    new_samples_z = torch.randn(num_new_samples, latent_dim)\n",
    "    new_samples_c = torch.ones(num_new_samples, dtype=torch.long) * minority_class\n",
    "    new_samples_c = torch.nn.functional.one_hot(new_samples_c, num_classes=cond_dim).float()\n",
    "\n",
    "    new_time_points = np.tile(np.arange(original_num_time_steps), num_new_samples)\n",
    "    new_time_points_encoded = pd.get_dummies(new_time_points).values.reshape(num_new_samples, original_num_time_steps,\n",
    "                                                                            original_num_time_steps)\n",
    "    new_time_points_encoded_tensor = torch.tensor(new_time_points_encoded, dtype=torch.float32)\n",
    "\n",
    "    new_samples = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_new_samples):\n",
    "            sample = model.decode(new_samples_z[i].unsqueeze(0), new_samples_c[i].unsqueeze(0),\n",
    "                                  new_time_points_encoded_tensor[i].unsqueeze(0))\n",
    "            new_samples.append(sample.numpy())\n",
    "\n",
    "    new_samples_np = np.concatenate(new_samples, axis=0).reshape(num_new_samples, -1)\n",
    "\n",
    "    combined_X_train_all = np.concatenate([X_train_all, new_samples_np], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, np.full(num_new_samples, minority_class)], axis=0)\n",
    "\n",
    "    combined_X_train = combined_X_train_all[:, :2 * original_num_features]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n",
    "    rf.fit(combined_X_train, combined_y_train)\n",
    "\n",
    "    y_pred = rf.predict(X_val)\n",
    "    y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41eaeb5",
   "metadata": {},
   "source": [
    "### Kernel density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98925c60",
   "metadata": {},
   "source": [
    "Time-point weighted kernel function: When performing kernel density estimation, different weights are assigned to data at different time points. For example, a higher weight is given to data at a recent point in time and a lower weight to data at a distant point in time, by adjusting the weights to reflect the difference in importance of the points in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "259268b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "  Balanced_accuracy: 0.6381 ± 0.1725\n",
      "  Sensitivity: 0.5333 ± 0.3399\n",
      "  Specificity: 0.7429 ± 0.0728\n",
      "  AUC: 0.6976 ± 0.1969\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "np.random.seed(0)\n",
    "\n",
    "# Time Points weight\n",
    "weights = np.linspace(0.5, 1, original_num_time_steps)\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_all, y)):\n",
    "    X_train_all, X_val_all = X_all[train_idx], X_all[val_idx]\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Train KDE with all data (both y=0 and y=1)\n",
    "    all_X = X_train_all.reshape(-1, original_num_time_steps, original_num_features)\n",
    "    all_y = y_train\n",
    "\n",
    "    # Generate new samples using only data with y=1\n",
    "    minority_class = 1\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    minority_X = X_train_all[minority_indices].reshape(-1, original_num_time_steps, original_num_features)\n",
    "\n",
    "    new_samples = []\n",
    "\n",
    "    # Kernel density estimation and sampling of data for each time point\n",
    "    for time_point in range(original_num_time_steps):\n",
    "        time_point_data = all_X[:, time_point, :]\n",
    "\n",
    "        # Multiply the data for certain tp by the corresponding weights\n",
    "        weighted_time_point_data = time_point_data * weights[time_point]\n",
    "\n",
    "        # Perform kernel density estimation\n",
    "        kde = gaussian_kde(weighted_time_point_data.T)\n",
    "\n",
    "        minority_time_point_data = minority_X[:, time_point, :]\n",
    "        weighted_minority_time_point_data = minority_time_point_data * weights[time_point]\n",
    "\n",
    "        # Generate new samples by sampling from the distribution of kernel density estimates\n",
    "        new_samples_time_point_flattened = kde.resample(num_new_samples).T / weights[time_point]\n",
    "\n",
    "        new_samples.append(new_samples_time_point_flattened)\n",
    "\n",
    "    new_samples = np.stack(new_samples, axis=1).reshape(num_new_samples, -1)\n",
    "\n",
    "    combined_X_train_all = np.concatenate([X_train_all, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, np.full(num_new_samples, minority_class)], axis=0)\n",
    "\n",
    "    combined_X_train = combined_X_train_all[:, :2 * original_num_features]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n",
    "    rf.fit(combined_X_train, combined_y_train)\n",
    "\n",
    "    y_pred = rf.predict(X_val)\n",
    "    y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca29bf7d",
   "metadata": {},
   "source": [
    "### Dirichlet Distribution + Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9d05a",
   "metadata": {},
   "source": [
    "Time-point conditional sampling: when generating new samples from a sampling from the Dirichlet distribution, the probability distribution of the samples is adjusted depending on the time point. Sampling is performed at different time steps using different parameters of the Delikeray distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "721cf307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "  Balanced_accuracy: 0.6167 ± 0.1643\n",
      "  Sensitivity: 0.3333 ± 0.3651\n",
      "  Specificity: 0.9000 ± 0.0571\n",
      "  AUC: 0.6548 ± 0.1745\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "np.random.seed(0)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_all, y)):\n",
    "    X_train_all, X_val_all = X_all[train_idx], X_all[val_idx]\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    all_X = X_train_all.reshape(-1, original_num_time_steps, original_num_features)\n",
    "    all_y = y_train\n",
    "\n",
    "    minority_class = 1\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    minority_X = X_train_all[minority_indices].reshape(-1, original_num_time_steps, original_num_features)\n",
    "\n",
    "    new_samples = []\n",
    "\n",
    "    for time_point in range(original_num_time_steps):\n",
    "        time_point_data = all_X[:, time_point, :]\n",
    "\n",
    "        # Calculate the mean and standard deviation of each feature for Bayesian inference to adjust the Dirichlet parameters\n",
    "        feature_means = np.mean(time_point_data, axis=0)\n",
    "        feature_stds = np.std(time_point_data, axis=0)\n",
    "\n",
    "        # Adjust the parameters of the Dirichlet distribution based on Bayesian inference\n",
    "        # Ensure that all elements in the alpha array are greater than 0\n",
    "        alpha = np.maximum(feature_means + 1, 1e-8)  # Minimal：1e-8\n",
    "\n",
    "        minority_time_point_data = minority_X[:, time_point, :]\n",
    "\n",
    "        #  Sampling from the Delicacy distribution to generate new samples\n",
    "        samples_at_time_point = np.random.dirichlet(alpha, num_new_samples)\n",
    "\n",
    "        new_samples.append(samples_at_time_point)\n",
    "\n",
    "    new_samples = np.stack(new_samples, axis=1).reshape(num_new_samples, -1)\n",
    "\n",
    "    combined_X_train_all = np.concatenate([X_train_all, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, np.full(num_new_samples, minority_class)], axis=0)\n",
    "\n",
    "    combined_X_train = combined_X_train_all[:, :2 * original_num_features]\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n",
    "    rf.fit(combined_X_train, combined_y_train)\n",
    "\n",
    "    y_pred = rf.predict(X_val)\n",
    "    y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d67000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc7e500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
