{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53cf98dd",
   "metadata": {},
   "source": [
    "Based on the full data subset A with no missing data, the full samples for y=0 and y=1 are learned and new samples are generated using the data and labels from tp2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1ccfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from skbio.stats.composition import clr, alr, ilr\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, cut_tree\n",
    "from scipy.spatial.distance import pdist, cdist\n",
    "from scipy.stats.mstats import gmean\n",
    "from scipy.stats import gaussian_kde, gamma, dirichlet, multivariate_normal\n",
    "from scipy.special import softmax\n",
    "import scipy.stats as stats\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b82e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGaussianMixtureModel:\n",
    "    \n",
    "    def __init__(self, n_components=2, max_iter=100, tol=1e-3, random_state=None): \n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    def _initialize_parameters(self, X, C):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_conditions = C.shape[1]\n",
    "        \n",
    "        # Initialize using K-means\n",
    "        kmeans = KMeans(n_clusters=self.n_components, random_state=self.random_state)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Initialize weight parameters (related to conditional variables)\n",
    "        self.weight_coeffs = np.random.randn(self.n_components, n_conditions + 1)\n",
    "        \n",
    "        # Initialize mean parameters (related to conditional variables)\n",
    "        self.mean_coeffs = np.zeros((self.n_components, n_features, n_conditions + 1))\n",
    "        for k in range(self.n_components):\n",
    "            mask = (labels == k)\n",
    "            if np.sum(mask) > 0:\n",
    "                self.mean_coeffs[k, :, 0] = np.mean(X[mask], axis=0)\n",
    "        \n",
    "        # Initialize covariance matrices (assume they're independent of conditional variables for now)\n",
    "        self.covariances = np.zeros((self.n_components, n_features, n_features))\n",
    "        for k in range(self.n_components):\n",
    "            mask = (labels == k)\n",
    "            if np.sum(mask) > 0:\n",
    "                diff = X[mask] - self.mean_coeffs[k, :, 0]\n",
    "                self.covariances[k] = np.dot(diff.T, diff) / np.sum(mask)\n",
    "            else:\n",
    "                self.covariances[k] = np.eye(n_features)\n",
    "                \n",
    "        # Ensure covariance matrices are positive definite\n",
    "        for k in range(self.n_components):\n",
    "            self.covariances[k] += 1e-6 * np.eye(n_features)\n",
    "    \n",
    "    def _e_step(self, X, C):\n",
    "        \"\"\"\n",
    "        E-step: Calculate posterior probability of each sample belonging to each component\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        log_prob = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        # Calculate weight for each component (dependent on conditional variables)\n",
    "        C_bias = np.hstack([np.ones((n_samples, 1)), C])\n",
    "        weights = self._compute_weights(C_bias)\n",
    "        \n",
    "        # Calculate mean for each component (dependent on conditional variables)\n",
    "        means = self._compute_means(C_bias)\n",
    "        \n",
    "        # Calculate log probability of each sample under each component\n",
    "        for k in range(self.n_components):\n",
    "            for i in range(n_samples):\n",
    "                log_prob[i, k] = np.log(weights[i, k] + 1e-10) + multivariate_normal.logpdf(\n",
    "                    X[i], mean=means[i, k], cov=self.covariances[k], allow_singular=True)\n",
    "        \n",
    "        # Calculate posterior probabilities (responsibilities)\n",
    "        log_prob_max = np.max(log_prob, axis=1, keepdims=True)\n",
    "        log_prob -= log_prob_max\n",
    "        prob = np.exp(log_prob)\n",
    "        responsibilities = prob / np.sum(prob, axis=1, keepdims=True)\n",
    "        \n",
    "        return responsibilities\n",
    "    \n",
    "    def _m_step(self, X, C, responsibilities):\n",
    "        \"\"\"\n",
    "        M-step: Update model parameters\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_conditions = C.shape[1]\n",
    "        \n",
    "        # Add bias term for M-step\n",
    "        C_bias = np.hstack([np.ones((n_samples, 1)), C])\n",
    "        \n",
    "        # Update weight coefficients\n",
    "        for k in range(self.n_components):\n",
    "            # Set up weighted least squares problem\n",
    "            W = np.sqrt(responsibilities[:, k, np.newaxis])\n",
    "            y = W\n",
    "            X_weighted = W * C_bias\n",
    "            \n",
    "            # Solve weighted least squares problem\n",
    "            self.weight_coeffs[k] = np.linalg.lstsq(X_weighted, y, rcond=None)[0].flatten()\n",
    "        \n",
    "        # Update mean coefficients\n",
    "        for k in range(self.n_components):\n",
    "            for d in range(n_features):\n",
    "                # Set up weighted least squares problem\n",
    "                W = np.sqrt(responsibilities[:, k, np.newaxis])\n",
    "                y = W * X[:, d, np.newaxis]\n",
    "                X_weighted = W * C_bias\n",
    "                \n",
    "                # Solve weighted least squares problem\n",
    "                self.mean_coeffs[k, d] = np.linalg.lstsq(X_weighted, y, rcond=None)[0].flatten()\n",
    "        \n",
    "        # Update covariance matrices (assume they're independent of conditional variables for now)\n",
    "        means = self._compute_means(C_bias)\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            self.covariances[k] = np.zeros((n_features, n_features))\n",
    "            weighted_sum = 0\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                diff = X[i] - means[i, k]\n",
    "                self.covariances[k] += responsibilities[i, k] * np.outer(diff, diff)\n",
    "                weighted_sum += responsibilities[i, k]\n",
    "            \n",
    "            self.covariances[k] /= (weighted_sum + 1e-10)\n",
    "            \n",
    "            # Ensure covariance matrices are positive definite\n",
    "            self.covariances[k] += 1e-6 * np.eye(n_features)\n",
    "    \n",
    "    def _compute_weights(self, C_bias):\n",
    "        \"\"\"\n",
    "        Calculate weight for each sample under each component\n",
    "        \"\"\"\n",
    "        n_samples = C_bias.shape[0]\n",
    "        logits = np.dot(C_bias, self.weight_coeffs.T)\n",
    "        \n",
    "        # Use softmax to ensure weights are positive and sum to 1\n",
    "        logits_max = np.max(logits, axis=1, keepdims=True)\n",
    "        logits -= logits_max\n",
    "        exp_logits = np.exp(logits)\n",
    "        weights = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def _compute_means(self, C_bias):\n",
    "        \"\"\"\n",
    "        Calculate mean for each sample under each component\n",
    "        \"\"\"\n",
    "        n_samples = C_bias.shape[0]\n",
    "        n_features = self.mean_coeffs.shape[1]\n",
    "        \n",
    "        means = np.zeros((n_samples, self.n_components, n_features))\n",
    "        for k in range(self.n_components):\n",
    "            for d in range(n_features):\n",
    "                means[:, k, d] = np.dot(C_bias, self.mean_coeffs[k, d])\n",
    "                \n",
    "        return means\n",
    "    \n",
    "    def fit(self, X, C):\n",
    "        \"\"\"\n",
    "        Fit conditional Gaussian mixture model using EM algorithm\n",
    "        \"\"\"\n",
    "        # Initialize parameters\n",
    "        self._initialize_parameters(X, C)\n",
    "        \n",
    "        # EM algorithm iterations\n",
    "        prev_log_likelihood = -np.inf\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # E-step: Calculate posterior probabilities\n",
    "            responsibilities = self._e_step(X, C)\n",
    "            \n",
    "            # M-step: Update parameters\n",
    "            self._m_step(X, C, responsibilities)\n",
    "            \n",
    "            # Calculate log likelihood\n",
    "            C_bias = np.hstack([np.ones((X.shape[0], 1)), C])\n",
    "            weights = self._compute_weights(C_bias)\n",
    "            means = self._compute_means(C_bias)\n",
    "            \n",
    "            log_likelihood = 0\n",
    "            for i in range(X.shape[0]):\n",
    "                component_log_probs = []\n",
    "                for k in range(self.n_components):\n",
    "                    component_log_prob = np.log(weights[i, k] + 1e-10) + multivariate_normal.logpdf(\n",
    "                        X[i], mean=means[i, k], cov=self.covariances[k], allow_singular=True)\n",
    "                    component_log_probs.append(component_log_prob)\n",
    "                \n",
    "                log_likelihood += self._log_sum_exp(np.array(component_log_probs))\n",
    "            \n",
    "            # Check for convergence\n",
    "            if abs(log_likelihood - prev_log_likelihood) < self.tol:\n",
    "                break\n",
    "                \n",
    "            prev_log_likelihood = log_likelihood\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def _log_sum_exp(self, log_probs):\n",
    "        \"\"\"\n",
    "        Calculate log-sum-exp to avoid numerical overflow\n",
    "        \"\"\"\n",
    "        max_log_prob = np.max(log_probs)\n",
    "        return max_log_prob + np.log(np.sum(np.exp(log_probs - max_log_prob)))\n",
    "    \n",
    "    def predict(self, X, C):\n",
    "        \"\"\"\n",
    "        Predict cluster labels for samples\n",
    "        \"\"\"\n",
    "        responsibilities = self._e_step(X, C)\n",
    "        return np.argmax(responsibilities, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X, C):\n",
    "        \"\"\"\n",
    "        Predict probability of samples belonging to each component\n",
    "        \"\"\"\n",
    "        return self._e_step(X, C)\n",
    "    \n",
    "    def sample(self, C, n_samples=1):\n",
    "        \"\"\"\n",
    "        Generate samples based on conditional variables\n",
    "        \"\"\"\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        \n",
    "        C_bias = np.hstack([np.ones((C.shape[0], 1)), C])\n",
    "        \n",
    "        # Calculate mixture weights\n",
    "        weights = self._compute_weights(C_bias)\n",
    "        \n",
    "        # Calculate mean for each component\n",
    "        means = self._compute_means(C_bias)\n",
    "        \n",
    "        # Generate samples\n",
    "        samples = []\n",
    "        component_labels = []\n",
    "        \n",
    "        samples_per_condition = n_samples // C.shape[0]\n",
    "        remainder = n_samples % C.shape[0]\n",
    "        \n",
    "        for i in range(C.shape[0]):\n",
    "            # Determine number of samples to generate for current condition\n",
    "            n_current = samples_per_condition + (1 if i < remainder else 0)\n",
    "            \n",
    "            # Randomly select components\n",
    "            k_samples = np.random.choice(\n",
    "                self.n_components, \n",
    "                size=n_current, \n",
    "                p=weights[i]\n",
    "            )\n",
    "            \n",
    "            # Generate samples from each selected component\n",
    "            for k in k_samples:\n",
    "                sample = np.random.multivariate_normal(\n",
    "                    means[i, k], \n",
    "                    self.covariances[k]\n",
    "                )\n",
    "                samples.append(sample)\n",
    "                component_labels.append(k)\n",
    "                \n",
    "        return np.array(samples), np.array(component_labels)\n",
    "\n",
    "def conditional_gmm(X_train, y_train, tp2_train, minority_class=1, num_new_samples=100, n_components=2, random_state=42):\n",
    "    # Get indices of minority class samples\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    \n",
    "    # Create conditional variables: tp2 data + labels\n",
    "    C = np.column_stack([tp2_train, y_train.reshape(-1, 1)])\n",
    "    \n",
    "    # Create and fit conditional Gaussian mixture model\n",
    "    cgmm = ConditionalGaussianMixtureModel(n_components=n_components, random_state=random_state)\n",
    "    cgmm.fit(X_train, C)\n",
    "    \n",
    "    # Prepare conditions for generating samples\n",
    "    # Method 1: Use conditions from all minority class samples\n",
    "    minority_conditions = C[minority_indices]\n",
    "    \n",
    "    # If number of minority samples is less than requested new samples, reuse conditions\n",
    "    if len(minority_indices) < num_new_samples:\n",
    "        # Calculate number of repeats needed\n",
    "        repeat_times = num_new_samples // len(minority_indices) + 1\n",
    "        minority_conditions = np.tile(minority_conditions, (repeat_times, 1))[:num_new_samples]\n",
    "    else:\n",
    "        # Randomly select conditions from minority class samples\n",
    "        random_indices = np.random.choice(len(minority_indices), num_new_samples, replace=False)\n",
    "        minority_conditions = minority_conditions[random_indices]\n",
    "    \n",
    "    # Generate new samples using conditions\n",
    "    new_samples, _ = cgmm.sample(minority_conditions, n_samples=num_new_samples)\n",
    "    \n",
    "    # Label all generated samples as minority class\n",
    "    new_labels = np.full(num_new_samples, minority_class)\n",
    "    \n",
    "    return new_samples, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c94de462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_pred_proba):\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp)\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    return {\n",
    "#         'Accuracy': accuracy,\n",
    "        'Balanced_accuracy': balanced_accuracy,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "#         'F1 Score': f1,\n",
    "        'AUC': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2202c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eb3d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = \"0\"\n",
    "    \n",
    "set_all_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b804814",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27b4ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-fold cross-validation\n",
    "# 1. There are positive samples in both the training and validation sets.\n",
    "# 2. the class distributions in the training and validation sets are similar to the original dataset.\n",
    "\n",
    "n_splits = 5\n",
    "num_seeds = 5\n",
    "skf = StratifiedKFold(n_splits= n_splits, shuffle=True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc040e25",
   "metadata": {},
   "source": [
    "### metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b094fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata = pd.read_csv(\"data/BASIC_metadata_full.csv\", sep=',', low_memory=False)\n",
    "Metadata = Metadata.drop(Metadata.columns[0], axis=1)\n",
    "\n",
    "# convert timepoits to 0,1,2\n",
    "Metadata.loc[Metadata.TimePoint == \"Trimester2\",\"TimePoint\"] = 0 \n",
    "Metadata.loc[Metadata.TimePoint == \"Trimester3\",\"TimePoint\"] = 1\n",
    "Metadata.loc[Metadata.TimePoint == \"PostpartumWeek6\",\"TimePoint\"] = 2\n",
    "\n",
    "# turn insufficient reads to NaN\n",
    "i = Metadata[Metadata.ReadsNumber < 500000].index\n",
    "Metadata.loc[i, 'ReadsNumber'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8dc56",
   "metadata": {},
   "source": [
    "### species data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f473d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile =pd.read_csv(\"data/Species_Profile_full.csv\",sep=',',low_memory=False)\n",
    "\n",
    "# extract all bacteria names\n",
    "full_list_bacteria = list(profile.columns)[1:]\n",
    "\n",
    "species = profile.to_numpy()[:,1:]\n",
    "\n",
    "species_num = np.shape(species)[1] # 713 species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9729239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join profile and metadeata\n",
    "merged_data_base = pd.merge(profile, Metadata, left_on='Sample_id', right_on='Sample_ID')\n",
    "\n",
    "merged_data = merged_data_base.dropna(subset=['ReadsNumber'])[['Individual_ID', 'TimePoint', 'EPDS', 'Dichotomous_EPDS'] + full_list_bacteria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b0a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sample individuals whose EPDS != NaN at tp2\n",
    "individuals_with_na_epds_at_tp2 = merged_data[\n",
    "    (merged_data['TimePoint'] == 2) & (merged_data['EPDS'].isna())\n",
    "]['Individual_ID'].unique()\n",
    "\n",
    "data = merged_data[~merged_data['Individual_ID'].isin(individuals_with_na_epds_at_tp2)]\n",
    "\n",
    "# 2. Sample individuals with data at tp0, tp1 and tp2\n",
    "individuals_with_all_timepoints = data.groupby('Individual_ID').filter(lambda x: set(x['TimePoint']) >= {0, 1, 2})['Individual_ID'].unique()\n",
    "data = data[data['Individual_ID'].isin(individuals_with_all_timepoints)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52c09730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that have a value of 0 at all time points for all samples\n",
    "columns_to_drop = []\n",
    "for col in full_list_bacteria:\n",
    "    if (data[col] == 0).all():\n",
    "        columns_to_drop.append(col)\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Update full_list_bacteria\n",
    "full_list_bacteria = [col for col in full_list_bacteria if col not in columns_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0e84f",
   "metadata": {},
   "source": [
    "### （1）Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "374f4ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Replace the zero value by 1/2 of the non - zero minimum value\n",
    "data[full_list_bacteria] = data[full_list_bacteria].astype(float)\n",
    "\n",
    "# Replace with 1/2 of the non-zero minimum value of the row\n",
    "matrix = data[full_list_bacteria].values\n",
    "for i in range(matrix.shape[0]):\n",
    "    row = matrix[i, :]\n",
    "    non_zero_values = row[~np.isnan(row) & (row > 0)]\n",
    "    if len(non_zero_values) > 0:\n",
    "        min_non_zero = np.min(non_zero_values)\n",
    "        half_min = min_non_zero / 2\n",
    "        row[row == 0] = half_min\n",
    "    matrix[i, :] = row\n",
    "\n",
    "data[full_list_bacteria] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c52966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "for i in range(matrix.shape[0]):\n",
    "    row = matrix[i, :]\n",
    "    \n",
    "    row_sum = np.sum(row)\n",
    "    row = row / row_sum\n",
    "    matrix[i, :] = row\n",
    "\n",
    "data[full_list_bacteria] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53d65d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data from [individual * tp, feature] into [individual, tp, feature]\n",
    "grouped = data.groupby('Individual_ID')\n",
    "\n",
    "transformed_data = []\n",
    "labels = []\n",
    "\n",
    "for individual_id, group in grouped:\n",
    "    time_point_matrix = np.full((3, len(full_list_bacteria)), np.nan)\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        time_point = int(row['TimePoint'])\n",
    "        time_point_matrix[time_point] = row[full_list_bacteria].values\n",
    "\n",
    "    tp2_row = group[group['TimePoint'] == 2]\n",
    "    label = tp2_row['Dichotomous_EPDS'].values[0]\n",
    "\n",
    "    transformed_data.append(time_point_matrix)\n",
    "    labels.append(label)\n",
    "\n",
    "transformed_data = np.array(transformed_data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "735a337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority class： 15\n",
      "majority class： 70\n"
     ]
    }
   ],
   "source": [
    "print(\"minority class：\", len(labels[labels == 1]))\n",
    "print(\"majority class：\", len(labels[labels == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02162531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "  Balanced_accuracy: 0.5000 ± 0.0000\n",
      "  Sensitivity: 0.0000 ± 0.0000\n",
      "  Specificity: 1.0000 ± 0.0000\n",
      "  AUC: 0.5871 ± 0.1759\n"
     ]
    }
   ],
   "source": [
    "# Build an RF model on the data to make predictions with only CLR.\n",
    "# There are a lot of NAs in features\n",
    "X = transformed_data[:, :2, :].reshape(transformed_data.shape[0], -1)\n",
    "y = labels\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32012d48",
   "metadata": {},
   "source": [
    "### （2）Feature Extraction + OverSampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77599732",
   "metadata": {},
   "source": [
    "OverSampling is performed using cGMM as the bottom, at which time no data transformations are performed to compare the effectiveness of the feature extraction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00d7594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(train_data, k):\n",
    "    selected_features_tp0 = []\n",
    "    selected_features_tp1 = []\n",
    "    \n",
    "    for time_point in [0, 1]:\n",
    "        time_point_data = train_data[train_data['TimePoint'] == time_point]\n",
    "        \n",
    "        X = time_point_data.drop(['Individual_ID', 'TimePoint', 'EPDS', 'Dichotomous_EPDS'], axis=1)\n",
    "        y = time_point_data['Dichotomous_EPDS']\n",
    "\n",
    "        selector = SelectKBest(score_func=f_classif, k=k) \n",
    "        X_new = selector.fit_transform(X, y)\n",
    "\n",
    "        feature_indices = selector.get_support(indices=True)\n",
    "        feature_columns = X.columns[feature_indices]\n",
    "        \n",
    "        if time_point == 0:\n",
    "            selected_features_tp0 = feature_columns\n",
    "        elif time_point == 1:\n",
    "            selected_features_tp1 = feature_columns\n",
    "            \n",
    "    feature_columns = list(set(selected_features_tp0) | set(selected_features_tp1))\n",
    "    print(f\"Selected features: {feature_columns}\")\n",
    "    \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94252db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_tp2(train_data, k):\n",
    "    selected_features_tp0 = []\n",
    "    selected_features_tp1 = []\n",
    "    \n",
    "    tp2_labels = train_data[train_data['TimePoint'] == 2][['Individual_ID', 'Dichotomous_EPDS']]\n",
    "    \n",
    "    for time_point in [0, 1]:\n",
    "        time_point_data = train_data[train_data['TimePoint'] == time_point]\n",
    "        \n",
    "        merged_data = time_point_data.merge(tp2_labels, on='Individual_ID', suffixes=('', '_tp2'))\n",
    "\n",
    "        X = merged_data.drop(['Individual_ID', 'TimePoint', 'EPDS', 'Dichotomous_EPDS', 'Dichotomous_EPDS_tp2'], axis=1)\n",
    "        y = merged_data['Dichotomous_EPDS_tp2'] \n",
    "\n",
    "        selector = SelectKBest(score_func=f_classif, k=k) \n",
    "        X_new = selector.fit_transform(X, y)\n",
    "\n",
    "        feature_indices = selector.get_support(indices=True)\n",
    "        feature_columns = X.columns[feature_indices]\n",
    "        \n",
    "        if time_point == 0:\n",
    "            selected_features_tp0 = feature_columns\n",
    "        elif time_point == 1:\n",
    "            selected_features_tp1 = feature_columns\n",
    "            \n",
    "    feature_columns = list(set(selected_features_tp0) | set(selected_features_tp1))\n",
    "    print(f\"Selected features: {feature_columns}\")\n",
    "    \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c75bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracted_datasets(data, feature_columns):\n",
    "    unused_bacteria_columns = [col for col in full_list_bacteria if col not in feature_columns]\n",
    "    others_column = data[unused_bacteria_columns].sum(axis=1).to_frame(name='Others')\n",
    "    \n",
    "    extracted_data = data[['Individual_ID', 'TimePoint', 'EPDS', 'Dichotomous_EPDS'] + list(feature_columns)]\n",
    "    extracted_data = pd.concat([extracted_data, others_column], axis=1)\n",
    "    \n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ebb401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_and_extract_labels(extracted_data, feature_columns):\n",
    "    # turn data from [individual * tp, feature] into [individual, tp, feature]\n",
    "    grouped = extracted_data.groupby('Individual_ID')\n",
    "\n",
    "    extracted_data_final = []\n",
    "    labels = []\n",
    "\n",
    "    for individual_id, group in grouped:\n",
    "        time_point_matrix = np.full((3, len(feature_columns)), np.nan)\n",
    "\n",
    "        for _, row in group.iterrows():\n",
    "            time_point = int(row['TimePoint'])\n",
    "            time_point_matrix[time_point] = row[feature_columns].values\n",
    "\n",
    "        tp2_row = group[group['TimePoint'] == 2]\n",
    "        label = tp2_row['Dichotomous_EPDS'].values[0]\n",
    "\n",
    "        extracted_data_final.append(time_point_matrix)\n",
    "        labels.append(label)\n",
    "\n",
    "    extracted_data_final = np.array(extracted_data_final)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return extracted_data_final, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d78f0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the number of new samples generated\n",
    "num_new_samples = 55 # 70-15 = 55\n",
    "original_num_time_steps = 2\n",
    "minority_class = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0cc07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_ids = np.unique(data['Individual_ID'])\n",
    "y_for_fold = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33db403",
   "metadata": {},
   "source": [
    "### GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0338c",
   "metadata": {},
   "source": [
    "#### (0) Non-Featurn Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15a698da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "  Balanced_accuracy: 0.4990 ± 0.0397\n",
      "  Sensitivity: 0.0667 ± 0.1333\n",
      "  Specificity: 0.9314 ± 0.0892\n",
      "  AUC: 0.5719 ± 0.1289\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "X = transformed_data[:, :2, :].reshape(transformed_data.shape[0], -1)\n",
    "y = labels\n",
    "original_num_features = transformed_data.shape[2]\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    minority_class = 1\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    X_train_minority = X_train[minority_indices]\n",
    "\n",
    "    gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "    gmm.fit(X_train_minority)\n",
    "\n",
    "    new_samples_flattened, _ = gmm.sample(num_new_samples)\n",
    "    \n",
    "    new_samples_np = new_samples_flattened.reshape(new_samples_flattened.shape[0], original_num_time_steps, original_num_features)\n",
    "    new_samples_np = new_samples_np.reshape(new_samples_flattened.shape[0], -1)\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples_np], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, np.full(new_samples_np.shape[0], minority_class)], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbdaa83",
   "metadata": {},
   "source": [
    "#### (1) Based on tp0&1 data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8f05bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Pseudoflavonifractor_sp_An184', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Streptococcus_sp_F0442', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Streptococcus_vestibularis', 'Coprococcus_eutactus', 'Lactococcus_lactis']\n",
      "Selected features: ['Faecalitalea_cylindroides', 'Sellimonas_intestinalis', 'Roseburia_sp_CAG_303', 'Clostridium_perfringens', 'Bifidobacterium_animalis', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Megamonas_funiformis_CAG_377', 'Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Enterococcus_faecalis', 'Denitrobacterium_detoxificans', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Aeriscardovia_aeriphila', 'Enterococcus_faecium', 'Bacteroides_sp_CAG_443', 'Blautia_producta', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Lactococcus_lactis']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.5290 ± 0.1524\n",
      "  Sensitivity: 0.1867 ± 0.2680\n",
      "  Specificity: 0.8714 ± 0.0857\n",
      "  AUC: 0.6252 ± 0.1938\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "\n",
    "    train_data = data[data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = data[data['Individual_ID'].isin(val_ids)]\n",
    "\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "\n",
    "    extracted_data = extracted_datasets(data, feature_columns)\n",
    "    \n",
    "    feature_columns = feature_columns + ['Others']\n",
    "\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data, feature_columns)\n",
    "\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "    \n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    original_num_features = extracted_data_final.shape[2]\n",
    "\n",
    "    minority_class = 1\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    X_train_minority = X_train[minority_indices]\n",
    "\n",
    "    gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "    gmm.fit(X_train_minority)\n",
    "\n",
    "    new_samples_flattened, _ = gmm.sample(num_new_samples)\n",
    "    \n",
    "    new_samples_np = new_samples_flattened.reshape(new_samples_flattened.shape[0], original_num_time_steps, original_num_features)\n",
    "    new_samples_np = new_samples_np.reshape(new_samples_flattened.shape[0], -1) \n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples_np], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, np.full(new_samples_np.shape[0], minority_class)], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8641ffb1",
   "metadata": {},
   "source": [
    "#### (2) Based on the data from tp0&1 and the labels from tp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd9aeea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Delftia_acidovorans', 'Candidatus_Gastranaerophilales_bacterium', 'Enterococcus_faecalis', 'Eubacterium_sp_OM08_24', 'Bifidobacterium_dentium', 'Eisenbergiella_massiliensis', 'Coprococcus_eutactus']\n",
      "Selected features: ['Bifidobacterium_breve', 'Leuconostoc_mesenteroides', 'Roseburia_hominis', 'Candidatus_Gastranaerophilales_bacterium', 'Enterococcus_faecalis', 'Bifidobacterium_longum', 'Blautia_wexlerae', 'Coprococcus_eutactus']\n",
      "Selected features: ['Salmonella_enterica', 'Bacillus_intestinalis', 'Roseburia_hominis', 'Enterococcus_faecalis', 'Eisenbergiella_massiliensis', 'Coprococcus_eutactus', 'Veillonella_rogosae']\n",
      "Selected features: ['Megamonas_funiformis_CAG_377', 'Bacillus_intestinalis', 'Ruminococcus_callidus', 'Bifidobacterium_longum', 'Actinomyces_sp_ICM47', 'Blautia_wexlerae', 'Coprococcus_eutactus']\n",
      "Selected features: ['Coprococcus_eutactus', 'Salmonella_enterica', 'Bacillus_intestinalis', 'Allisonella_histaminiformans', 'Lactobacillus_rogosae', 'Lachnospira_pectinoschiza']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.5181 ± 0.1151\n",
      "  Sensitivity: 0.1333 ± 0.1886\n",
      "  Specificity: 0.9029 ± 0.1028\n",
      "  AUC: 0.5319 ± 0.1996\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "\n",
    "    train_data = data[data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = data[data['Individual_ID'].isin(val_ids)]\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "\n",
    "    feature_columns = extract_features_tp2(extracted_data_base, k=4)\n",
    "\n",
    "    extracted_data = extracted_datasets(data, feature_columns)\n",
    "    \n",
    "    feature_columns = feature_columns + ['Others']\n",
    "\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data, feature_columns)\n",
    "\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "    \n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    original_num_features = extracted_data_final.shape[2]\n",
    "\n",
    "    minority_class = 1\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    X_train_minority = X_train[minority_indices]\n",
    "\n",
    "    gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "    gmm.fit(X_train_minority)\n",
    "\n",
    "    new_samples_flattened, _ = gmm.sample(num_new_samples)\n",
    "    \n",
    "    new_samples_np = new_samples_flattened.reshape(new_samples_flattened.shape[0], original_num_time_steps, original_num_features)\n",
    "    new_samples_np = new_samples_np.reshape(new_samples_flattened.shape[0], -1) \n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples_np], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, np.full(new_samples_np.shape[0], minority_class)], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef3f66d",
   "metadata": {},
   "source": [
    "### cGMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c35ea",
   "metadata": {},
   "source": [
    "#### (3) Based on tp0&1 data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28cb29ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Pseudoflavonifractor_sp_An184', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Streptococcus_sp_F0442', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Streptococcus_vestibularis', 'Coprococcus_eutactus', 'Lactococcus_lactis']\n",
      "Selected features: ['Faecalitalea_cylindroides', 'Sellimonas_intestinalis', 'Roseburia_sp_CAG_303', 'Clostridium_perfringens', 'Bifidobacterium_animalis', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Megamonas_funiformis_CAG_377', 'Faecalitalea_cylindroides', 'Enterococcus_faecium', 'Enterococcus_faecalis', 'Denitrobacterium_detoxificans', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica']\n",
      "Selected features: ['Aeriscardovia_aeriphila', 'Enterococcus_faecium', 'Bacteroides_sp_CAG_443', 'Blautia_producta', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Lactococcus_lactis']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.5543 ± 0.1542\n",
      "  Sensitivity: 0.2400 ± 0.3058\n",
      "  Specificity: 0.8686 ± 0.1044\n",
      "  AUC: 0.6029 ± 0.1780\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "    train_data = data[data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = data[data['Individual_ID'].isin(val_ids)]\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "    extracted_data = extracted_datasets(data, feature_columns)\n",
    "    \n",
    "    feature_columns = feature_columns + ['Others']\n",
    "    \n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data, feature_columns)\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "    \n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1)\n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    \n",
    "    minority_class = 1\n",
    "    \n",
    "    new_samples, new_labels = conditional_gmm(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        tp2_train,\n",
    "        minority_class=minority_class,\n",
    "        num_new_samples=num_new_samples,\n",
    "    )\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, new_labels], axis=0)\n",
    "    \n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2e431c",
   "metadata": {},
   "source": [
    "#### (4) Based on the data from tp0&1 and the labels from tp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3c123b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Delftia_acidovorans', 'Candidatus_Gastranaerophilales_bacterium', 'Enterococcus_faecalis', 'Eubacterium_sp_OM08_24', 'Bifidobacterium_dentium', 'Eisenbergiella_massiliensis', 'Coprococcus_eutactus']\n",
      "Selected features: ['Bifidobacterium_breve', 'Leuconostoc_mesenteroides', 'Roseburia_hominis', 'Candidatus_Gastranaerophilales_bacterium', 'Enterococcus_faecalis', 'Bifidobacterium_longum', 'Blautia_wexlerae', 'Coprococcus_eutactus']\n",
      "Selected features: ['Salmonella_enterica', 'Bacillus_intestinalis', 'Roseburia_hominis', 'Enterococcus_faecalis', 'Eisenbergiella_massiliensis', 'Coprococcus_eutactus', 'Veillonella_rogosae']\n",
      "Selected features: ['Megamonas_funiformis_CAG_377', 'Bacillus_intestinalis', 'Ruminococcus_callidus', 'Bifidobacterium_longum', 'Actinomyces_sp_ICM47', 'Blautia_wexlerae', 'Coprococcus_eutactus']\n",
      "Selected features: ['Coprococcus_eutactus', 'Salmonella_enterica', 'Bacillus_intestinalis', 'Allisonella_histaminiformans', 'Lactobacillus_rogosae', 'Lachnospira_pectinoschiza']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.5514 ± 0.0913\n",
      "  Sensitivity: 0.2000 ± 0.1633\n",
      "  Specificity: 0.9029 ± 0.0635\n",
      "  AUC: 0.5357 ± 0.2065\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "    train_data = data[data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = data[data['Individual_ID'].isin(val_ids)]\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "    feature_columns = extract_features_tp2(extracted_data_base, k=4)\n",
    "    extracted_data = extracted_datasets(data, feature_columns)\n",
    "    \n",
    "    feature_columns = feature_columns + ['Others']\n",
    "    \n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data, feature_columns)\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "    \n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1)\n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    \n",
    "    minority_class = 1\n",
    "    \n",
    "    new_samples, new_labels = conditional_gmm(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        tp2_train,\n",
    "        minority_class=minority_class,\n",
    "        num_new_samples=num_new_samples,\n",
    "    )\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, new_labels], axis=0)\n",
    "    \n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd8cb15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
