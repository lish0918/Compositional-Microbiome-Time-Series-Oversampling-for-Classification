{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53cf98dd",
   "metadata": {},
   "source": [
    "Based on the full data subset A with no missing data, the full samples for y=0 and y=1 are learned and new samples are generated using the data and labels from tp2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1ccfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from skbio.stats.composition import clr, alr, ilr\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats.mstats import gmean\n",
    "from scipy.stats import gaussian_kde\n",
    "import scipy.stats as stats\n",
    "import torch\n", 
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94de462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_pred_proba):\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp)\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    return {\n",
    "#         'Accuracy': accuracy,\n",
    "        'Balanced_accuracy': balanced_accuracy,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "#         'F1 Score': f1,\n",
    "        'AUC': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2202c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb3d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTHONHASHSEED\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b804814",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b4ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-fold cross-validation\n",
    "# 1. There are positive samples in both the training and validation sets.\n",
    "# 2. the class distributions in the training and validation sets are similar to the original dataset.\n",
    "\n",
    "n_splits = 5\n",
    "num_seeds = 5\n",
    "skf = StratifiedKFold(n_splits= n_splits, shuffle=True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc040e25",
   "metadata": {},
   "source": [
    "### metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b094fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata = pd.read_csv(\"data/BASIC_metadata_full.csv\", sep=',', low_memory=False)\n",
    "Metadata = Metadata.drop(Metadata.columns[0], axis=1)\n",
    "\n",
    "# convert timepoits to 0,1,2\n",
    "Metadata.loc[Metadata.TimePoint == \"Trimester2\",\"TimePoint\"] = 0 \n",
    "Metadata.loc[Metadata.TimePoint == \"Trimester3\",\"TimePoint\"] = 1\n",
    "Metadata.loc[Metadata.TimePoint == \"PostpartumWeek6\",\"TimePoint\"] = 2\n",
    "\n",
    "# turn insufficient reads to NaN\n",
    "i = Metadata[Metadata.ReadsNumber < 500000].index\n",
    "Metadata.loc[i, 'ReadsNumber'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8dc56",
   "metadata": {},
   "source": [
    "### species data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f473d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile =pd.read_csv(\"data/Species_Profile_full.csv\",sep=',',low_memory=False)\n",
    "\n",
    "# extract all bacteria names\n",
    "full_list_bacteria = list(profile.columns)[1:]\n",
    "\n",
    "species = profile.to_numpy()[:,1:]\n",
    "\n",
    "species_num = np.shape(species)[1] # 713 species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9729239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join profile and metadeata\n",
    "merged_data_base = pd.merge(profile, Metadata, left_on='Sample_id', right_on='Sample_ID')\n",
    "\n",
    "merged_data = merged_data_base.dropna(subset=['ReadsNumber'])[['Individual_ID', 'TimePoint', 'EPDS', 'Dichotomous_EPDS'] + full_list_bacteria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09b0a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sample individuals whose EPDS != NaN at tp2\n",
    "individuals_with_na_epds_at_tp2 = merged_data[\n",
    "    (merged_data['TimePoint'] == 2) & (merged_data['EPDS'].isna())\n",
    "]['Individual_ID'].unique()\n",
    "\n",
    "data = merged_data[~merged_data['Individual_ID'].isin(individuals_with_na_epds_at_tp2)]\n",
    "\n",
    "# 2. Sample individuals with data at tp0, tp1 and tp2\n",
    "individuals_with_all_timepoints = data.groupby('Individual_ID').filter(lambda x: set(x['TimePoint']) >= {0, 1, 2})['Individual_ID'].unique()\n",
    "data = data[data['Individual_ID'].isin(individuals_with_all_timepoints)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c09730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that have a value of 0 at all time points for all samples\n",
    "columns_to_drop = []\n",
    "for col in full_list_bacteria:\n",
    "    if (data[col] == 0).all():\n",
    "        columns_to_drop.append(col)\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Update full_list_bacteria\n",
    "full_list_bacteria = [col for col in full_list_bacteria if col not in columns_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0e84f",
   "metadata": {},
   "source": [
    "### Zero-Replacing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c96676",
   "metadata": {},
   "source": [
    "### Comparison of Zero-value Replacement Methods\n",
    "\n",
    "#### Replacement with half of the non-zero minimum value of each row\n",
    "- **Effect description**: <span style=\"color:red;\"> The effect after feature extraction was poor compared to replacing with column, but significant after applying oversampling.<span>\n",
    "- **Applicable scenario**: Suitable for scenarios where the focus is on within-sample variation, such as comparing the relative abundances of different species within the same sample.\n",
    "- **Advantages**: Maintains the internal structure of the sample and reduces the impact of inter-sample variation.\n",
    "- **Disadvantages**: May lead to inconsistent replacement values across different samples, affecting cross-sample comparisons.\n",
    "\n",
    "---\n",
    "\n",
    "#### Replacement with half of the non-zero minimum value of each column\n",
    "- **Effect description**: <span style=\"color:red;\"> Oversampling fails to learn and generate new and better samples.<span>\n",
    "- **Applicable scenario**: Applicable when the focus is on the distribution of features (e.g., species) across different samples.\n",
    "- **Advantages**: Maintains the consistency of features across different samples, facilitating cross-sample comparisons.\n",
    "- **Disadvantages**: May ignore the internal structure of the sample, affecting the analysis of within-sample variation.\n",
    "\n",
    "---\n",
    "\n",
    "#### Replacement with default value 1e-10\n",
    "- **Effect description**: <span style=\"color:red;\"> The effect after feature extraction was poor compared to replacing with column, and also oversampling fails to learn and generate new and better samples.<span>\n",
    "- **Applicable scenario**: Suitable when zero values result from measurement limitations and the replacement value minimally impacts overall analysis, e.g., in preliminary data exploration.\n",
    "- **Advantages**: imple to implement, no complex calculations or parameter estimations needed. Quickly handles zero values for further processing.\n",
    "- **Disadvantages**: Ignores data characteristics like non - zero value distributions. Fixed replacement may not reflect true zero - valued data, affecting analysis accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e61fdd",
   "metadata": {},
   "source": [
    "OverSampling is performed using GMM as the base, when no feature selection or data transformation is performed to compare the role of zero-value replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3000ed",
   "metadata": {},
   "source": [
    "#### (0) No Replacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "828fac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[full_list_bacteria] = data[full_list_bacteria].astype(float)\n",
    "\n",
    "# Replace with 1/2 of the non-zero minimum value of the row\n",
    "new_data = data.copy()\n",
    "matrix = new_data[full_list_bacteria].values\n",
    "\n",
    "# Normalization\n",
    "for i in range(matrix.shape[0]):\n",
    "    row = matrix[i, :]\n",
    "    \n",
    "    row_sum = np.sum(row)\n",
    "    row = row / row_sum\n",
    "    matrix[i, :] = row\n",
    "\n",
    "new_data[full_list_bacteria] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "085e0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data from [individual * tp, feature] into [individual, tp, feature]\n",
    "grouped = new_data.groupby('Individual_ID')\n",
    "\n",
    "transformed_data = []\n",
    "labels = []\n",
    "\n",
    "for individual_id, group in grouped:\n",
    "    time_point_matrix = np.full((3, len(full_list_bacteria)), np.nan)\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        time_point = int(row['TimePoint'])\n",
    "        time_point_matrix[time_point] = row[full_list_bacteria].values\n",
    "\n",
    "    tp2_row = group[group['TimePoint'] == 2]\n",
    "    label = tp2_row['Dichotomous_EPDS'].values[0]\n",
    "\n",
    "    transformed_data.append(time_point_matrix)\n",
    "    labels.append(label)\n",
    "\n",
    "transformed_data = np.array(transformed_data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae1fc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transformed_data[:, :2, :].reshape(transformed_data.shape[0], -1)\n",
    "y = labels\n",
    "\n",
    "individual_ids = np.unique(new_data['Individual_ID'])\n",
    "\n",
    "# Assuming the number of new samples generated\n",
    "num_new_samples = 55 # 70-15 = 55\n",
    "original_num_time_steps = 2\n",
    "original_num_features = transformed_data.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d658698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "  Balanced_accuracy: 0.5000 ± 0.0000\n",
      "  Sensitivity: 0.0000 ± 0.0000\n",
      "  Specificity: 1.0000 ± 0.0000\n",
      "  AUC: 0.3876 ± 0.2099\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y)):\n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    minority_class = 1\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    X_train_minority = X_train[minority_indices]\n",
    "\n",
    "    gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "    gmm.fit(X_train_minority)\n",
    "\n",
    "    new_samples_flattened, _ = gmm.sample(num_new_samples)\n",
    "    \n",
    "    new_samples_np = new_samples_flattened.reshape(new_samples_flattened.shape[0], original_num_time_steps, original_num_features)\n",
    "    new_samples_np = new_samples_np.reshape(new_samples_flattened.shape[0], -1)\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples_np], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, np.full(new_samples_np.shape[0], minority_class)], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12bdcb",
   "metadata": {},
   "source": [
    "#### (1) Based on Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "374f4ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice 1：based on row\n",
    "# Replace with 1/2 of the non-zero minimum value of the row\n",
    "new_data = data.copy()\n",
    "matrix = new_data[full_list_bacteria].values\n",
    "\n",
    "for i in range(matrix.shape[0]):\n",
    "    row = matrix[i, :]\n",
    "    non_zero_values = row[~np.isnan(row) & (row > 0)]\n",
    "    if len(non_zero_values) > 0:\n",
    "        min_non_zero = np.min(non_zero_values)\n",
    "        half_min = min_non_zero / 2\n",
    "        row[row == 0] = half_min\n",
    "    \n",
    "    matrix[i, :] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc253690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "for i in range(matrix.shape[0]):\n",
    "    row = matrix[i, :]\n",
    "    \n",
    "    row_sum = np.sum(row)\n",
    "    row = row / row_sum\n",
    "    matrix[i, :] = row\n",
    "\n",
    "new_data[full_list_bacteria] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "537f4a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data from [individual * tp, feature] into [individual, tp, feature]\n",
    "grouped = new_data.groupby('Individual_ID')\n",
    "\n",
    "transformed_data = []\n",
    "labels = []\n",
    "\n",
    "for individual_id, group in grouped:\n",
    "    time_point_matrix = np.full((3, len(full_list_bacteria)), np.nan)\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        time_point = int(row['TimePoint'])\n",
    "        time_point_matrix[time_point] = row[full_list_bacteria].values\n",
    "\n",
    "    tp2_row = group[group['TimePoint'] == 2]\n",
    "    label = tp2_row['Dichotomous_EPDS'].values[0]\n",
    "\n",
    "    transformed_data.append(time_point_matrix)\n",
    "    labels.append(label)\n",
    "\n",
    "transformed_data = np.array(transformed_data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "937b2011",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transformed_data[:, :2, :].reshape(transformed_data.shape[0], -1)\n",
    "y = labels\n",
    "\n",
    "individual_ids = np.unique(new_data['Individual_ID'])\n",
    "\n",
    "# Assuming the number of new samples generated\n",
    "num_new_samples = 55 # 70-15 = 55\n",
    "original_num_time_steps = 2\n",
    "original_num_features = transformed_data.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "812b48a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "  Balanced_accuracy: 0.5000 ± 0.0000\n",
      "  Sensitivity: 0.0000 ± 0.0000\n",
      "  Specificity: 1.0000 ± 0.0000\n",
      "  AUC: 0.5871 ± 0.1759\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y)):\n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87ba9265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "  Balanced_accuracy: 0.4990 ± 0.0397\n",
      "  Sensitivity: 0.0667 ± 0.1333\n",
      "  Specificity: 0.9314 ± 0.0892\n",
      "  AUC: 0.5719 ± 0.1289\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y)):\n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    minority_class = 1\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    X_train_minority = X_train[minority_indices]\n",
    "\n",
    "    gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "    gmm.fit(X_train_minority)\n",
    "\n",
    "    new_samples_flattened, _ = gmm.sample(num_new_samples)\n",
    "    \n",
    "    new_samples_np = new_samples_flattened.reshape(new_samples_flattened.shape[0], original_num_time_steps, original_num_features)\n",
    "    new_samples_np = new_samples_np.reshape(new_samples_flattened.shape[0], -1)\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples_np], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, np.full(new_samples_np.shape[0], minority_class)], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8b700",
   "metadata": {},
   "source": [
    "#### (2) Based on Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e214db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice 2：based on column\n",
    "# 1.Replace the zero value by 1/2 of the non - zero minimum value\n",
    "new_data = data.copy()\n",
    "matrix = new_data[full_list_bacteria].values\n",
    "\n",
    "for col_index in range(matrix.shape[1]):\n",
    "    col = matrix[:, col_index]\n",
    "    non_zero_values = col[~np.isnan(col) & (col > 0)]\n",
    "    if len(non_zero_values) > 0:\n",
    "        min_non_zero = np.min(non_zero_values)\n",
    "        half_min = min_non_zero / 2\n",
    "        col[col == 0] = half_min\n",
    "    else:\n",
    "        col[col == 0] = 1e-10\n",
    "\n",
    "    matrix[:, col_index] = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b514834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "for i in range(matrix.shape[0]):\n",
    "    row = matrix[i, :]\n",
    "    \n",
    "    row_sum = np.sum(row)\n",
    "    row = row / row_sum\n",
    "    matrix[i, :] = row\n",
    "\n",
    "new_data[full_list_bacteria] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4402a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data from [individual * tp, feature] into [individual, tp, feature]\n",
    "grouped = new_data.groupby('Individual_ID')\n",
    "\n",
    "transformed_data = []\n",
    "labels = []\n",
    "\n",
    "for individual_id, group in grouped:\n",
    "    time_point_matrix = np.full((3, len(full_list_bacteria)), np.nan)\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        time_point = int(row['TimePoint'])\n",
    "        time_point_matrix[time_point] = row[full_list_bacteria].values\n",
    "\n",
    "    tp2_row = group[group['TimePoint'] == 2]\n",
    "    label = tp2_row['Dichotomous_EPDS'].values[0]\n",
    "\n",
    "    transformed_data.append(time_point_matrix)\n",
    "    labels.append(label)\n",
    "\n",
    "transformed_data = np.array(transformed_data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43a32837",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transformed_data[:, :2, :].reshape(transformed_data.shape[0], -1)\n",
    "y = labels\n",
    "\n",
    "individual_ids = np.unique(new_data['Individual_ID'])\n",
    "\n",
    "# Assuming the number of new samples generated\n",
    "num_new_samples = 55 # 70-15 = 55\n",
    "original_num_time_steps = 2\n",
    "original_num_features = transformed_data.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "137515f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "  Balanced_accuracy: 0.4829 ± 0.0178\n",
      "  Sensitivity: 0.0000 ± 0.0000\n",
      "  Specificity: 0.9657 ± 0.0357\n",
      "  AUC: 0.4005 ± 0.1263\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y)):\n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    minority_class = 1\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    X_train_minority = X_train[minority_indices]\n",
    "\n",
    "    gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "    gmm.fit(X_train_minority)\n",
    "\n",
    "    new_samples_flattened, _ = gmm.sample(num_new_samples)\n",
    "    \n",
    "    new_samples_np = new_samples_flattened.reshape(new_samples_flattened.shape[0], original_num_time_steps, original_num_features)\n",
    "    new_samples_np = new_samples_np.reshape(new_samples_flattened.shape[0], -1)\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples_np], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, np.full(new_samples_np.shape[0], minority_class)], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b223f95",
   "metadata": {},
   "source": [
    "#### (3) Default Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "587aac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice 3：set a default\n",
    "# 1.Replace the zero value by default\n",
    "new_data = data.copy()\n",
    "matrix = new_data[full_list_bacteria].values\n",
    "default_value = 1e-10\n",
    "\n",
    "for col_index in range(matrix.shape[1]):\n",
    "    col = matrix[:, col_index]\n",
    "\n",
    "    col[col == 0] = default_value\n",
    "\n",
    "    matrix[:, col_index] = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c52966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "for i in range(matrix.shape[0]):\n",
    "    row = matrix[i, :]\n",
    "    \n",
    "    row_sum = np.sum(row)\n",
    "    row = row / row_sum\n",
    "    matrix[i, :] = row\n",
    "\n",
    "new_data[full_list_bacteria] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53d65d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data from [individual * tp, feature] into [individual, tp, feature]\n",
    "grouped = new_data.groupby('Individual_ID')\n",
    "\n",
    "transformed_data = []\n",
    "labels = []\n",
    "\n",
    "for individual_id, group in grouped:\n",
    "    time_point_matrix = np.full((3, len(full_list_bacteria)), np.nan)\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        time_point = int(row['TimePoint'])\n",
    "        time_point_matrix[time_point] = row[full_list_bacteria].values\n",
    "\n",
    "    tp2_row = group[group['TimePoint'] == 2]\n",
    "    label = tp2_row['Dichotomous_EPDS'].values[0]\n",
    "\n",
    "    transformed_data.append(time_point_matrix)\n",
    "    labels.append(label)\n",
    "\n",
    "transformed_data = np.array(transformed_data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d78f0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transformed_data[:, :2, :].reshape(transformed_data.shape[0], -1)\n",
    "y = labels\n",
    "\n",
    "individual_ids = np.unique(new_data['Individual_ID'])\n",
    "\n",
    "# Assuming the number of new samples generated\n",
    "num_new_samples = 55 # 70-15 = 55\n",
    "original_num_time_steps = 2\n",
    "original_num_features = transformed_data.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4225b6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "  Balanced_accuracy: 0.4914 ± 0.0183\n",
      "  Sensitivity: 0.0000 ± 0.0000\n",
      "  Specificity: 0.9829 ± 0.0366\n",
      "  AUC: 0.2705 ± 0.1578\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y)):\n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    minority_class = 1\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    X_train_minority = X_train[minority_indices]\n",
    "\n",
    "    gmm = GaussianMixture(n_components=2, random_state=42)\n",
    "    gmm.fit(X_train_minority)\n",
    "\n",
    "    new_samples_flattened, _ = gmm.sample(num_new_samples)\n",
    "    \n",
    "    new_samples_np = new_samples_flattened.reshape(new_samples_flattened.shape[0], original_num_time_steps, original_num_features)\n",
    "    new_samples_np = new_samples_np.reshape(new_samples_flattened.shape[0], -1)\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples_np], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, np.full(new_samples_np.shape[0], minority_class)], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c123b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
