{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53cf98dd",
   "metadata": {},
   "source": [
    "Based on the full data subset A with no missing data, the full samples for y=0 and y=1 are learned and new samples are generated using the data and labels from tp2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1ccfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import f_classif, SelectKBest\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from skbio.stats.composition import clr, alr, ilr\n",
    "from scipy.cluster.hierarchy import linkage, fcluster, cut_tree\n",
    "from scipy.spatial.distance import pdist, cdist\n",
    "from scipy.stats.mstats import gmean\n",
    "from scipy.stats import gaussian_kde, gamma, dirichlet, multivariate_normal\n",
    "from scipy.special import softmax\n",
    "import scipy.stats as stats\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f9ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGaussianMixtureModel:\n",
    "    \n",
    "    def __init__(self, n_components=2, max_iter=100, tol=1e-3, random_state=None): \n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    def _initialize_parameters(self, X, C):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_conditions = C.shape[1]\n",
    "        \n",
    "        # Initialize using K-means\n",
    "        kmeans = KMeans(n_clusters=self.n_components, random_state=self.random_state)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Initialize weight parameters (related to conditional variables)\n",
    "        self.weight_coeffs = np.random.randn(self.n_components, n_conditions + 1)\n",
    "        \n",
    "        # Initialize mean parameters (related to conditional variables)\n",
    "        self.mean_coeffs = np.zeros((self.n_components, n_features, n_conditions + 1))\n",
    "        for k in range(self.n_components):\n",
    "            mask = (labels == k)\n",
    "            if np.sum(mask) > 0:\n",
    "                self.mean_coeffs[k, :, 0] = np.mean(X[mask], axis=0)\n",
    "        \n",
    "        # Initialize covariance matrices (assume they're independent of conditional variables for now)\n",
    "        self.covariances = np.zeros((self.n_components, n_features, n_features))\n",
    "        for k in range(self.n_components):\n",
    "            mask = (labels == k)\n",
    "            if np.sum(mask) > 0:\n",
    "                diff = X[mask] - self.mean_coeffs[k, :, 0]\n",
    "                self.covariances[k] = np.dot(diff.T, diff) / np.sum(mask)\n",
    "            else:\n",
    "                self.covariances[k] = np.eye(n_features)\n",
    "                \n",
    "        # Ensure covariance matrices are positive definite\n",
    "        for k in range(self.n_components):\n",
    "            self.covariances[k] += 1e-6 * np.eye(n_features)\n",
    "    \n",
    "    def _e_step(self, X, C):\n",
    "        \"\"\"\n",
    "        E-step: Calculate posterior probability of each sample belonging to each component\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        log_prob = np.zeros((n_samples, self.n_components))\n",
    "        \n",
    "        # Calculate weight for each component (dependent on conditional variables)\n",
    "        C_bias = np.hstack([np.ones((n_samples, 1)), C])\n",
    "        weights = self._compute_weights(C_bias)\n",
    "        \n",
    "        # Calculate mean for each component (dependent on conditional variables)\n",
    "        means = self._compute_means(C_bias)\n",
    "        \n",
    "        # Calculate log probability of each sample under each component\n",
    "        for k in range(self.n_components):\n",
    "            for i in range(n_samples):\n",
    "                log_prob[i, k] = np.log(weights[i, k] + 1e-10) + multivariate_normal.logpdf(\n",
    "                    X[i], mean=means[i, k], cov=self.covariances[k], allow_singular=True)\n",
    "        \n",
    "        # Calculate posterior probabilities (responsibilities)\n",
    "        log_prob_max = np.max(log_prob, axis=1, keepdims=True)\n",
    "        log_prob -= log_prob_max\n",
    "        prob = np.exp(log_prob)\n",
    "        responsibilities = prob / np.sum(prob, axis=1, keepdims=True)\n",
    "        \n",
    "        return responsibilities\n",
    "    \n",
    "    def _m_step(self, X, C, responsibilities):\n",
    "        \"\"\"\n",
    "        M-step: Update model parameters\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_conditions = C.shape[1]\n",
    "        \n",
    "        # Add bias term for M-step\n",
    "        C_bias = np.hstack([np.ones((n_samples, 1)), C])\n",
    "        \n",
    "        # Update weight coefficients\n",
    "        for k in range(self.n_components):\n",
    "            # Set up weighted least squares problem\n",
    "            W = np.sqrt(responsibilities[:, k, np.newaxis])\n",
    "            y = W\n",
    "            X_weighted = W * C_bias\n",
    "            \n",
    "            # Solve weighted least squares problem\n",
    "            self.weight_coeffs[k] = np.linalg.lstsq(X_weighted, y, rcond=None)[0].flatten()\n",
    "        \n",
    "        # Update mean coefficients\n",
    "        for k in range(self.n_components):\n",
    "            for d in range(n_features):\n",
    "                # Set up weighted least squares problem\n",
    "                W = np.sqrt(responsibilities[:, k, np.newaxis])\n",
    "                y = W * X[:, d, np.newaxis]\n",
    "                X_weighted = W * C_bias\n",
    "                \n",
    "                # Solve weighted least squares problem\n",
    "                self.mean_coeffs[k, d] = np.linalg.lstsq(X_weighted, y, rcond=None)[0].flatten()\n",
    "        \n",
    "        # Update covariance matrices (assume they're independent of conditional variables for now)\n",
    "        means = self._compute_means(C_bias)\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            self.covariances[k] = np.zeros((n_features, n_features))\n",
    "            weighted_sum = 0\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                diff = X[i] - means[i, k]\n",
    "                self.covariances[k] += responsibilities[i, k] * np.outer(diff, diff)\n",
    "                weighted_sum += responsibilities[i, k]\n",
    "            \n",
    "            self.covariances[k] /= (weighted_sum + 1e-10)\n",
    "            \n",
    "            # Ensure covariance matrices are positive definite\n",
    "            self.covariances[k] += 1e-6 * np.eye(n_features)\n",
    "    \n",
    "    def _compute_weights(self, C_bias):\n",
    "        \"\"\"\n",
    "        Calculate weight for each sample under each component\n",
    "        \"\"\"\n",
    "        n_samples = C_bias.shape[0]\n",
    "        logits = np.dot(C_bias, self.weight_coeffs.T)\n",
    "        \n",
    "        # Use softmax to ensure weights are positive and sum to 1\n",
    "        logits_max = np.max(logits, axis=1, keepdims=True)\n",
    "        logits -= logits_max\n",
    "        exp_logits = np.exp(logits)\n",
    "        weights = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def _compute_means(self, C_bias):\n",
    "        \"\"\"\n",
    "        Calculate mean for each sample under each component\n",
    "        \"\"\"\n",
    "        n_samples = C_bias.shape[0]\n",
    "        n_features = self.mean_coeffs.shape[1]\n",
    "        \n",
    "        means = np.zeros((n_samples, self.n_components, n_features))\n",
    "        for k in range(self.n_components):\n",
    "            for d in range(n_features):\n",
    "                means[:, k, d] = np.dot(C_bias, self.mean_coeffs[k, d])\n",
    "                \n",
    "        return means\n",
    "    \n",
    "    def fit(self, X, C):\n",
    "        \"\"\"\n",
    "        Fit conditional Gaussian mixture model using EM algorithm\n",
    "        \"\"\"\n",
    "        # Initialize parameters\n",
    "        self._initialize_parameters(X, C)\n",
    "        \n",
    "        # EM algorithm iterations\n",
    "        prev_log_likelihood = -np.inf\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # E-step: Calculate posterior probabilities\n",
    "            responsibilities = self._e_step(X, C)\n",
    "            \n",
    "            # M-step: Update parameters\n",
    "            self._m_step(X, C, responsibilities)\n",
    "            \n",
    "            # Calculate log likelihood\n",
    "            C_bias = np.hstack([np.ones((X.shape[0], 1)), C])\n",
    "            weights = self._compute_weights(C_bias)\n",
    "            means = self._compute_means(C_bias)\n",
    "            \n",
    "            log_likelihood = 0\n",
    "            for i in range(X.shape[0]):\n",
    "                component_log_probs = []\n",
    "                for k in range(self.n_components):\n",
    "                    component_log_prob = np.log(weights[i, k] + 1e-10) + multivariate_normal.logpdf(\n",
    "                        X[i], mean=means[i, k], cov=self.covariances[k], allow_singular=True)\n",
    "                    component_log_probs.append(component_log_prob)\n",
    "                \n",
    "                log_likelihood += self._log_sum_exp(np.array(component_log_probs))\n",
    "            \n",
    "            # Check for convergence\n",
    "            if abs(log_likelihood - prev_log_likelihood) < self.tol:\n",
    "                break\n",
    "                \n",
    "            prev_log_likelihood = log_likelihood\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def _log_sum_exp(self, log_probs):\n",
    "        \"\"\"\n",
    "        Calculate log-sum-exp to avoid numerical overflow\n",
    "        \"\"\"\n",
    "        max_log_prob = np.max(log_probs)\n",
    "        return max_log_prob + np.log(np.sum(np.exp(log_probs - max_log_prob)))\n",
    "    \n",
    "    def predict(self, X, C):\n",
    "        \"\"\"\n",
    "        Predict cluster labels for samples\n",
    "        \"\"\"\n",
    "        responsibilities = self._e_step(X, C)\n",
    "        return np.argmax(responsibilities, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X, C):\n",
    "        \"\"\"\n",
    "        Predict probability of samples belonging to each component\n",
    "        \"\"\"\n",
    "        return self._e_step(X, C)\n",
    "    \n",
    "    def sample(self, C, n_samples=1):\n",
    "        \"\"\"\n",
    "        Generate samples based on conditional variables\n",
    "        \"\"\"\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        \n",
    "        C_bias = np.hstack([np.ones((C.shape[0], 1)), C])\n",
    "        \n",
    "        # Calculate mixture weights\n",
    "        weights = self._compute_weights(C_bias)\n",
    "        \n",
    "        # Calculate mean for each component\n",
    "        means = self._compute_means(C_bias)\n",
    "        \n",
    "        # Generate samples\n",
    "        samples = []\n",
    "        component_labels = []\n",
    "        \n",
    "        samples_per_condition = n_samples // C.shape[0]\n",
    "        remainder = n_samples % C.shape[0]\n",
    "        \n",
    "        for i in range(C.shape[0]):\n",
    "            # Determine number of samples to generate for current condition\n",
    "            n_current = samples_per_condition + (1 if i < remainder else 0)\n",
    "            \n",
    "            # Randomly select components\n",
    "            k_samples = np.random.choice(\n",
    "                self.n_components, \n",
    "                size=n_current, \n",
    "                p=weights[i]\n",
    "            )\n",
    "            \n",
    "            # Generate samples from each selected component\n",
    "            for k in k_samples:\n",
    "                sample = np.random.multivariate_normal(\n",
    "                    means[i, k], \n",
    "                    self.covariances[k]\n",
    "                )\n",
    "                samples.append(sample)\n",
    "                component_labels.append(k)\n",
    "                \n",
    "        return np.array(samples), np.array(component_labels)\n",
    "\n",
    "def conditional_gmm(X_train, y_train, tp2_train, minority_class=1, num_new_samples=100, n_components=2, random_state=42):\n",
    "    # Get indices of minority class samples\n",
    "    minority_indices = np.where(y_train == minority_class)[0]\n",
    "    \n",
    "    # Create conditional variables: tp2 data + labels\n",
    "    C = np.column_stack([tp2_train, y_train.reshape(-1, 1)])\n",
    "    \n",
    "    # Create and fit conditional Gaussian mixture model\n",
    "    cgmm = ConditionalGaussianMixtureModel(n_components=n_components, random_state=random_state)\n",
    "    cgmm.fit(X_train, C)\n",
    "    \n",
    "    # Prepare conditions for generating samples\n",
    "    # Method 1: Use conditions from all minority class samples\n",
    "    minority_conditions = C[minority_indices]\n",
    "    \n",
    "    # If number of minority samples is less than requested new samples, reuse conditions\n",
    "    if len(minority_indices) < num_new_samples:\n",
    "        # Calculate number of repeats needed\n",
    "        repeat_times = num_new_samples // len(minority_indices) + 1\n",
    "        minority_conditions = np.tile(minority_conditions, (repeat_times, 1))[:num_new_samples]\n",
    "    else:\n",
    "        # Randomly select conditions from minority class samples\n",
    "        random_indices = np.random.choice(len(minority_indices), num_new_samples, replace=False)\n",
    "        minority_conditions = minority_conditions[random_indices]\n",
    "    \n",
    "    # Generate new samples using conditions\n",
    "    new_samples, _ = cgmm.sample(minority_conditions, n_samples=num_new_samples)\n",
    "    \n",
    "    # Label all generated samples as minority class\n",
    "    new_labels = np.full(num_new_samples, minority_class)\n",
    "    \n",
    "    return new_samples, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c94de462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_pred_proba):\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    specificity = tn / (tn + fp)\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    return {\n",
    "#         'Accuracy': accuracy,\n",
    "        'Balanced_accuracy': balanced_accuracy,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "#         'F1 Score': f1,\n",
    "        'AUC': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2202c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eb3d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = \"0\"\n",
    "    \n",
    "set_all_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b804814",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27b4ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define K-fold cross-validation\n",
    "# 1. There are positive samples in both the training and validation sets.\n",
    "# 2. the class distributions in the training and validation sets are similar to the original dataset.\n",
    "\n",
    "n_splits = 5\n",
    "num_seeds = 5\n",
    "skf = StratifiedKFold(n_splits= n_splits, shuffle=True, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc040e25",
   "metadata": {},
   "source": [
    "### metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b094fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metadata = pd.read_csv(\"data/BASIC_metadata_full.csv\", sep=',', low_memory=False)\n",
    "Metadata = Metadata.drop(Metadata.columns[0], axis=1)\n",
    "\n",
    "# convert timepoits to 0,1,2\n",
    "Metadata.loc[Metadata.TimePoint == \"Trimester2\",\"TimePoint\"] = 0 \n",
    "Metadata.loc[Metadata.TimePoint == \"Trimester3\",\"TimePoint\"] = 1\n",
    "Metadata.loc[Metadata.TimePoint == \"PostpartumWeek6\",\"TimePoint\"] = 2\n",
    "\n",
    "# turn insufficient reads to NaN\n",
    "i = Metadata[Metadata.ReadsNumber < 500000].index\n",
    "Metadata.loc[i, 'ReadsNumber'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8dc56",
   "metadata": {},
   "source": [
    "### species data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f473d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile =pd.read_csv(\"data/Species_Profile_full.csv\",sep=',',low_memory=False)\n",
    "\n",
    "# extract all bacteria names\n",
    "full_list_bacteria = list(profile.columns)[1:]\n",
    "\n",
    "species = profile.to_numpy()[:,1:]\n",
    "\n",
    "species_num = np.shape(species)[1] # 713 species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9729239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join profile and metadeata\n",
    "merged_data_base = pd.merge(profile, Metadata, left_on='Sample_id', right_on='Sample_ID')\n",
    "\n",
    "merged_data = merged_data_base.dropna(subset=['ReadsNumber'])[['Individual_ID', 'TimePoint', 'EPDS', 'Dichotomous_EPDS'] + full_list_bacteria]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b0a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sample individuals whose EPDS != NaN at tp2\n",
    "individuals_with_na_epds_at_tp2 = merged_data[\n",
    "    (merged_data['TimePoint'] == 2) & (merged_data['EPDS'].isna())\n",
    "]['Individual_ID'].unique()\n",
    "\n",
    "data = merged_data[~merged_data['Individual_ID'].isin(individuals_with_na_epds_at_tp2)]\n",
    "\n",
    "# 2. Sample individuals with data at tp0, tp1 and tp2\n",
    "individuals_with_all_timepoints = data.groupby('Individual_ID').filter(lambda x: set(x['TimePoint']) >= {0, 1, 2})['Individual_ID'].unique()\n",
    "data = data[data['Individual_ID'].isin(individuals_with_all_timepoints)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52c09730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that have a value of 0 at all time points for all samples\n",
    "columns_to_drop = []\n",
    "for col in full_list_bacteria:\n",
    "    if (data[col] == 0).all():\n",
    "        columns_to_drop.append(col)\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# Update full_list_bacteria\n",
    "full_list_bacteria = [col for col in full_list_bacteria if col not in columns_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d0e84f",
   "metadata": {},
   "source": [
    "### （1）Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "374f4ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Replace the zero value by 1/2 of the non - zero minimum value\n",
    "data[full_list_bacteria] = data[full_list_bacteria].astype(float)\n",
    "\n",
    "# Replace with 1/2 of the non-zero minimum value of the row\n",
    "matrix = data[full_list_bacteria].values\n",
    "for i in range(matrix.shape[0]):\n",
    "    row = matrix[i, :]\n",
    "    non_zero_values = row[~np.isnan(row) & (row > 0)]\n",
    "    if len(non_zero_values) > 0:\n",
    "        min_non_zero = np.min(non_zero_values)\n",
    "        half_min = min_non_zero / 2\n",
    "        row[row == 0] = half_min\n",
    "    matrix[i, :] = row\n",
    "\n",
    "data[full_list_bacteria] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c52966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "for i in range(matrix.shape[0]):\n",
    "    row = matrix[i, :]\n",
    "    \n",
    "    row_sum = np.sum(row)\n",
    "    row = row / row_sum\n",
    "    matrix[i, :] = row\n",
    "\n",
    "data[full_list_bacteria] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a713fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data from [individual * tp, feature] into [individual, tp, feature]\n",
    "grouped = data.groupby('Individual_ID')\n",
    "\n",
    "transformed_data = []\n",
    "labels = []\n",
    "\n",
    "for individual_id, group in grouped:\n",
    "    time_point_matrix = np.full((3, len(full_list_bacteria)), np.nan)\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        time_point = int(row['TimePoint'])\n",
    "        time_point_matrix[time_point] = row[full_list_bacteria].values\n",
    "\n",
    "    tp2_row = group[group['TimePoint'] == 2]\n",
    "    label = tp2_row['Dichotomous_EPDS'].values[0]\n",
    "\n",
    "    transformed_data.append(time_point_matrix)\n",
    "    labels.append(label)\n",
    "\n",
    "transformed_data = np.array(transformed_data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6899d651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority class： 15\n",
      "majority class： 70\n"
     ]
    }
   ],
   "source": [
    "print(\"minority class：\", len(labels[labels == 1]))\n",
    "print(\"majority class：\", len(labels[labels == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ef30fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Results:\n",
      "  Balanced_accuracy: 0.5000 ± 0.0000\n",
      "  Sensitivity: 0.0000 ± 0.0000\n",
      "  Specificity: 1.0000 ± 0.0000\n",
      "  AUC: 0.5871 ± 0.1759\n"
     ]
    }
   ],
   "source": [
    "# Build an RF model on the data to make predictions with only CLR.\n",
    "# There are a lot of NAs in features\n",
    "X = transformed_data[:, :2, :].reshape(transformed_data.shape[0], -1)\n",
    "y = labels\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32012d48",
   "metadata": {},
   "source": [
    "### （2）Feature Extraction + OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00d7594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(train_data, k):\n",
    "    selected_features_tp0 = []\n",
    "    selected_features_tp1 = []\n",
    "    \n",
    "    for time_point in [0, 1]:\n",
    "        time_point_data = train_data[train_data['TimePoint'] == time_point]\n",
    "        \n",
    "        X = time_point_data.drop(['Individual_ID', 'TimePoint', 'EPDS', 'Dichotomous_EPDS'], axis=1)\n",
    "        y = time_point_data['Dichotomous_EPDS']\n",
    "\n",
    "        selector = SelectKBest(score_func=f_classif, k=k) \n",
    "        X_new = selector.fit_transform(X, y)\n",
    "\n",
    "        feature_indices = selector.get_support(indices=True)\n",
    "        feature_columns = X.columns[feature_indices]\n",
    "        \n",
    "        if time_point == 0:\n",
    "            selected_features_tp0 = feature_columns\n",
    "        elif time_point == 1:\n",
    "            selected_features_tp1 = feature_columns\n",
    "            \n",
    "    feature_columns = list(set(selected_features_tp0) | set(selected_features_tp1))\n",
    "    print(f\"Selected features: {feature_columns}\")\n",
    "    \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c75bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracted_datasets(data, feature_columns):\n",
    "    microbiome_columns = data.columns[4:]\n",
    "    \n",
    "    unused_bacteria_columns = [col for col in microbiome_columns if col not in feature_columns]\n",
    "    \n",
    "    others_column = data[unused_bacteria_columns].sum(axis=1).to_frame(name='Others')\n",
    "    \n",
    "    extracted_data = data[['Individual_ID', 'TimePoint', 'EPDS', 'Dichotomous_EPDS'] + feature_columns]\n",
    "    \n",
    "    extracted_data = pd.concat([extracted_data, others_column], axis=1)\n",
    "    \n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0c96a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clr_transform(data):\n",
    "    data_copy = data.copy()\n",
    "    id_columns = data_copy.columns[:4]\n",
    "    microbiome_columns = data_copy.columns[4:]\n",
    "\n",
    "    for _, group in data_copy.groupby('TimePoint'):\n",
    "        indices = group.index\n",
    "        group_data = group[microbiome_columns].values\n",
    "        clr_values = clr(group_data)\n",
    "        data_copy.loc[indices, microbiome_columns] = clr_values\n",
    "\n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c564e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alr_transform(data):\n",
    "    data_copy = data.copy()\n",
    "    id_columns = data_copy.columns[:4]\n",
    "    microbiome_columns = data_copy.columns[4:]\n",
    "\n",
    "    for _, group in data_copy.groupby('TimePoint'):\n",
    "        indices = group.index\n",
    "        group_data = group[microbiome_columns].values\n",
    "        # frist col as reference column\n",
    "        alr_transformed = alr(group_data, denominator_idx=0)\n",
    "        alr_df = pd.DataFrame(alr_transformed, columns=[col for i, col in enumerate(microbiome_columns) if i != 0], index=indices)\n",
    "        data_copy.loc[indices, microbiome_columns[1:]] = alr_df\n",
    "\n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d42526c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ilr_transform(data):\n",
    "    data_copy = data.copy()\n",
    "    id_columns = data_copy.columns[:4]\n",
    "    microbiome_columns = data_copy.columns[4:]\n",
    "\n",
    "    for _, group in data_copy.groupby('TimePoint'):\n",
    "        indices = group.index\n",
    "        group_data = group[microbiome_columns].values\n",
    "    \n",
    "        ilr_transformed = ilr(group_data)\n",
    "        ilr_df = pd.DataFrame(ilr_transformed, columns=[col for i, col in enumerate(microbiome_columns) if i != 0], index=indices)\n",
    "        data_copy.loc[indices, microbiome_columns[1:]] = ilr_df\n",
    "    \n",
    "    return data_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16d070ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchy_tree(Z, dY):  # each line of clusters is elements of new clusters obtained by hierarchy tree\n",
    "    clusters = -1 * np.ones((2 * dY - 1, dY))\n",
    "    clusters[0:dY, 0] = np.arange(dY)\n",
    "    clustersize = np.zeros(2 * dY - 1)\n",
    "    clustersize[0:dY] = np.ones(dY)\n",
    "    for i in range(0, dY - 1):\n",
    "        cl0, cl1 = int(Z[i, 0]), int(Z[i, 1])\n",
    "        s0, s1 = int(clustersize[cl0]), int(clustersize[cl1])\n",
    "        new_cluster = np.concatenate([clusters[cl0, 0:s0], clusters[cl1, 0:s1]])\n",
    "        clustersize[dY + i] = Z[i, 3]\n",
    "        clusters[dY + i, 0:int(clustersize[dY + i])] = new_cluster\n",
    "    return clustersize, clusters\n",
    "\n",
    "\n",
    "def data_transformation(X_, Z, clustersize, clusters, dY):  # pivot log ratios\n",
    "    stats = np.exp(np.mean(np.log(X_), axis=1, keepdims=True))  # geometric mean as the first feature\n",
    "    for i in range(dY - 2, -1, -1):\n",
    "        cl0, cl1 = int(Z[i, 0]), int(Z[i, 1])\n",
    "        s0, s1 = int(clustersize[cl0]), int(clustersize[cl1])\n",
    "        cluster0, cluster1 = clusters[cl0, 0:s0].astype(int), clusters[cl1, 0:s1].astype(int)\n",
    "        mean0, mean1 = np.mean(np.log(X_[:, cluster0]), axis=1), np.mean(np.log(X_[:, cluster1]), axis=1)\n",
    "        stats = np.append(stats, np.exp(mean0 - mean1).reshape(-1, 1), axis=1)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def plr_transform(data):\n",
    "    key_columns = data.columns[:4]\n",
    "    key_data = data[key_columns]\n",
    "    species_cols = data.columns[4:]\n",
    "    plr_features = []\n",
    "    for ind_id, ind_group in data.groupby('Individual_ID'):\n",
    "        for _, time_group in ind_group.groupby('TimePoint'):\n",
    "            x = time_group[species_cols].values\n",
    "            x_scaled = StandardScaler().fit_transform(x)\n",
    "            dist_matrix = pdist(x_scaled.T, metric='euclidean')\n",
    "            Z = linkage(dist_matrix, method='ward')\n",
    "            dY = len(species_cols)\n",
    "            clustersize, clusters = hierarchy_tree(Z, dY)\n",
    "            stats = data_transformation(x, Z, clustersize, clusters, dY)\n",
    "\n",
    "            record = {col: time_group[col].iloc[0] for col in key_columns}\n",
    "            for k in range(stats.shape[1]):\n",
    "                record[f'PLR_{k}'] = stats[0, k]\n",
    "            plr_features.append(pd.DataFrame(record, index=[0]))\n",
    "\n",
    "    plr_data = pd.concat(plr_features).reset_index(drop=True)\n",
    "    return plr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ebb401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_and_extract_labels(data):\n",
    "    feature_columns = data.columns[4:]\n",
    "    \n",
    "    grouped = data.groupby('Individual_ID')\n",
    "\n",
    "    extracted_data_final = []\n",
    "    labels = []\n",
    "\n",
    "    for individual_id, group in grouped:\n",
    "        time_point_matrix = np.full((3, len(feature_columns)), np.nan)\n",
    "\n",
    "        for _, row in group.iterrows():\n",
    "            time_point = int(row['TimePoint'])\n",
    "            time_point_matrix[time_point] = row[feature_columns].values\n",
    "\n",
    "        tp2_row = group[group['TimePoint'] == 2]\n",
    "        if not tp2_row.empty:  \n",
    "            label = tp2_row['Dichotomous_EPDS'].values[0]\n",
    "            labels.append(label)\n",
    "            extracted_data_final.append(time_point_matrix)\n",
    "\n",
    "    extracted_data_final = np.array(extracted_data_final)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return extracted_data_final, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d78f0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the number of new samples generated\n",
    "num_new_samples = 55 # 70-15 = 55\n",
    "minority_class = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0cc07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_ids = np.unique(data['Individual_ID'])\n",
    "y_for_fold = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46445e6",
   "metadata": {},
   "source": [
    "### A: Before Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ae657",
   "metadata": {},
   "source": [
    "##### CLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b37d43e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Bacteroides_sp_CAG_144', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Bacteroides_faecis', 'Paraprevotella_xylaniphila', 'Giardia_intestinalis']\n",
      "Selected features: ['Bacteroides_sp_CAG_144', 'Lactococcus_lactis', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Prevotella_sp_CAG_1185', 'Clostridium_methylpentosum', 'Sellimonas_intestinalis']\n",
      "Selected features: ['Bacteroides_sp_CAG_144', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Sellimonas_intestinalis', 'Prevotella_sp_CAG_1185', 'Clostridium_methylpentosum', 'Anaerotruncus_sp_CAG_528']\n",
      "Selected features: ['Bacteroides_sp_CAG_144', 'Bacteroides_salyersiae', 'Megamonas_funiformis_CAG_377', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Anaerotruncus_sp_CAG_528', 'Bacteroides_plebeius']\n",
      "Selected features: ['Prevotella_sp_CAG_1058', 'Blautia_producta', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Bacteroides_sp_CAG_443', 'Faecalitalea_cylindroides', 'Bifidobacterium_dentium']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.4671 ± 0.1217\n",
      "  Sensitivity: 0.1200 ± 0.2083\n",
      "  Specificity: 0.8143 ± 0.0969\n",
      "  AUC: 0.5590 ± 0.1803\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "clr_data = clr_transform(data)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "\n",
    "    train_data = clr_data[clr_data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = clr_data[clr_data['Individual_ID'].isin(val_ids)]\n",
    "\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "    \n",
    "    extracted_data = extracted_datasets(clr_data, feature_columns)\n",
    "\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data)\n",
    "\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "\n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1)\n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    \n",
    "    minority_class = 1\n",
    "    minority_count = np.sum(y_train == minority_class)\n",
    "    majority_count = np.sum(y_train != minority_class)\n",
    "\n",
    "    new_samples, new_labels = conditional_gmm(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        tp2_train,\n",
    "        minority_class=minority_class,\n",
    "        num_new_samples=num_new_samples,\n",
    "    )\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, new_labels], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b5e84e",
   "metadata": {},
   "source": [
    "##### PLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cf977ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['PLR_208', 'PLR_353', 'PLR_404', 'PLR_229', 'PLR_275', 'PLR_231', 'PLR_492', 'PLR_359']\n",
      "Selected features: ['PLR_343', 'PLR_525', 'PLR_283', 'PLR_296', 'PLR_404', 'PLR_231', 'PLR_419', 'PLR_492']\n",
      "Selected features: ['PLR_47', 'PLR_436', 'PLR_241', 'PLR_346', 'PLR_301', 'PLR_231', 'PLR_367', 'PLR_351']\n",
      "Selected features: ['PLR_208', 'PLR_283', 'PLR_436', 'PLR_301', 'PLR_275', 'PLR_332', 'PLR_492', 'PLR_457']\n",
      "Selected features: ['PLR_338', 'PLR_283', 'PLR_91', 'PLR_436', 'PLR_296', 'PLR_275', 'PLR_10', 'PLR_332']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.5176 ± 0.1376\n",
      "  Sensitivity: 0.1867 ± 0.2509\n",
      "  Specificity: 0.8486 ± 0.1075\n",
      "  AUC: 0.5657 ± 0.2037\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "plr_data = plr_transform(data)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "\n",
    "    train_data = plr_data[plr_data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = plr_data[plr_data['Individual_ID'].isin(val_ids)]\n",
    "\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "    \n",
    "    extracted_data = extracted_datasets(plr_data, feature_columns)\n",
    "\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data)\n",
    "\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "\n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1)\n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    \n",
    "    minority_class = 1\n",
    "    minority_count = np.sum(y_train == minority_class)\n",
    "    majority_count = np.sum(y_train != minority_class)\n",
    "\n",
    "    new_samples, new_labels = conditional_gmm(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        tp2_train,\n",
    "        minority_class=minority_class,\n",
    "        num_new_samples=num_new_samples,\n",
    "    )\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, new_labels], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508c6922",
   "metadata": {},
   "source": [
    "##### ALR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac284e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Bacteroides_sp_CAG_144', 'Bifidobacterium_bifidum', 'Allisonella_histaminiformans', 'Bacteroides_faecis', 'Paraprevotella_xylaniphila', 'Bacteroides_faecis_CAG_32']\n",
      "Selected features: ['Bacteroides_sp_CAG_144', 'Bifidobacterium_bifidum', 'Allisonella_histaminiformans', 'Bacteroides_faecis', 'Paraprevotella_xylaniphila', 'Clostridium_methylpentosum']\n",
      "Selected features: ['Bacteroides_salyersiae', 'Bacteroides_eggerthii', 'Bifidobacterium_bifidum', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Paraprevotella_xylaniphila', 'Prevotella_sp_CAG_1185']\n",
      "Selected features: ['Bacteroides_sp_CAG_144', 'Bacteroides_salyersiae', 'Eubacterium_sp_CAG_38', 'Bifidobacterium_bifidum', 'Allisonella_histaminiformans', 'Bacteroides_faecis', 'Bacteroides_plebeius']\n",
      "Selected features: ['Bacteroides_sp_CAG_144', 'Bacteroides_eggerthii', 'Allisonella_histaminiformans', 'Paraprevotella_xylaniphila', 'Bacteroides_sp_CAG_443', 'Bacteroides_faecis_CAG_32', 'Megamonas_funiformis']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.5171 ± 0.1260\n",
      "  Sensitivity: 0.2400 ± 0.2585\n",
      "  Specificity: 0.7943 ± 0.0680\n",
      "  AUC: 0.5424 ± 0.1286\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "alr_data = alr_transform(data)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "\n",
    "    train_data = alr_data[alr_data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = alr_data[alr_data['Individual_ID'].isin(val_ids)]\n",
    "\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "    \n",
    "    extracted_data = extracted_datasets(alr_data, feature_columns)\n",
    "\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data)\n",
    "\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "\n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1)\n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    \n",
    "    minority_class = 1\n",
    "    minority_count = np.sum(y_train == minority_class)\n",
    "    majority_count = np.sum(y_train != minority_class)\n",
    "\n",
    "    new_samples, new_labels = conditional_gmm(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        tp2_train,\n",
    "        minority_class=minority_class,\n",
    "        num_new_samples=num_new_samples,\n",
    "    )\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, new_labels], axis=0)\n",
    "    \n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c771d960",
   "metadata": {},
   "source": [
    "##### ILR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68bcc3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Gordonibacter_pamelaeae', 'Bacteroides_sp_CAG_144', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Bacteroides_faecis', 'Paraprevotella_xylaniphila', 'Giardia_intestinalis']\n",
      "Selected features: ['Bacteroides_sp_CAG_144', 'Lachnoclostridium_sp_An169', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Prevotella_sp_CAG_1185', 'Prevotella_sp_CAG_1058', 'Bifidobacterium_dentium']\n",
      "Selected features: ['Bacteroides_sp_CAG_144', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Sellimonas_intestinalis', 'Prevotella_sp_CAG_1185', 'Clostridium_sp_CAG_167', 'Anaerotruncus_sp_CAG_528']\n",
      "Selected features: ['Gordonibacter_pamelaeae', 'Bacteroides_sp_CAG_144', 'Bacteroides_salyersiae', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Anaerotruncus_sp_CAG_528', 'Bacteroides_plebeius']\n",
      "Selected features: ['Dorea_longicatena', 'Lachnoclostridium_sp_An169', 'Allisonella_histaminiformans', 'Bacteroides_sp_CAG_443', 'Prevotella_sp_CAG_1058', 'Bifidobacterium_dentium', 'Giardia_intestinalis']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.5662 ± 0.1821\n",
      "  Sensitivity: 0.3067 ± 0.2816\n",
      "  Specificity: 0.8257 ± 0.1566\n",
      "  AUC: 0.6176 ± 0.1814\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "ilr_data = ilr_transform(data)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "\n",
    "    train_data = ilr_data[ilr_data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = ilr_data[ilr_data['Individual_ID'].isin(val_ids)]\n",
    "\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "    \n",
    "    extracted_data = extracted_datasets(ilr_data, feature_columns)\n",
    "\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data)\n",
    "\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "\n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1)\n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    \n",
    "    minority_class = 1\n",
    "    minority_count = np.sum(y_train == minority_class)\n",
    "    majority_count = np.sum(y_train != minority_class)\n",
    "\n",
    "    new_samples, new_labels = conditional_gmm(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        tp2_train,\n",
    "        minority_class=minority_class,\n",
    "        num_new_samples=num_new_samples,\n",
    "    )\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, new_labels], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963b896",
   "metadata": {},
   "source": [
    "### B: After Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b1706",
   "metadata": {},
   "source": [
    "##### CLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26795be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Enterococcus_faecium', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Faecalitalea_cylindroides', 'Pseudoflavonifractor_sp_An184']\n",
      "Selected features: ['Coprococcus_eutactus', 'Roseburia_sp_CAG_303', 'Lactococcus_lactis', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Streptococcus_sp_F0442', 'Streptococcus_vestibularis']\n",
      "Selected features: ['Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Bifidobacterium_animalis', 'Butyricimonas_synergistica', 'Sellimonas_intestinalis', 'Faecalitalea_cylindroides', 'Clostridium_perfringens']\n",
      "Selected features: ['Denitrobacterium_detoxificans', 'Enterococcus_faecium', 'Megamonas_funiformis_CAG_377', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Faecalitalea_cylindroides']\n",
      "Selected features: ['Enterococcus_faecium', 'Lactococcus_lactis', 'Blautia_producta', 'Allisonella_histaminiformans', 'Enterococcus_faecalis', 'Bacteroides_sp_CAG_443', 'Aeriscardovia_aeriphila']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.6662 ± 0.0845\n",
      "  Sensitivity: 0.6267 ± 0.1960\n",
      "  Specificity: 0.7057 ± 0.1461\n",
      "  AUC: 0.6290 ± 0.1336\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "\n",
    "    train_data = data[data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = data[data['Individual_ID'].isin(val_ids)]\n",
    "\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "    \n",
    "    extracted_data = extracted_datasets(data, feature_columns)\n",
    "    \n",
    "    extracted_data = clr_transform(extracted_data)    \n",
    "\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data)\n",
    "\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "\n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1)\n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    \n",
    "    minority_class = 1\n",
    "    minority_count = np.sum(y_train == minority_class)\n",
    "    majority_count = np.sum(y_train != minority_class)\n",
    "\n",
    "    new_samples, new_labels = conditional_gmm(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        tp2_train,\n",
    "        minority_class=minority_class,\n",
    "        num_new_samples=num_new_samples,\n",
    "    )\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, new_labels], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f772bfe",
   "metadata": {},
   "source": [
    "##### PLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bddfde0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Enterococcus_faecium', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Faecalitalea_cylindroides', 'Pseudoflavonifractor_sp_An184']\n",
      "Selected features: ['Coprococcus_eutactus', 'Roseburia_sp_CAG_303', 'Lactococcus_lactis', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Streptococcus_sp_F0442', 'Streptococcus_vestibularis']\n",
      "Selected features: ['Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Bifidobacterium_animalis', 'Butyricimonas_synergistica', 'Sellimonas_intestinalis', 'Faecalitalea_cylindroides', 'Clostridium_perfringens']\n",
      "Selected features: ['Denitrobacterium_detoxificans', 'Enterococcus_faecium', 'Megamonas_funiformis_CAG_377', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Faecalitalea_cylindroides']\n",
      "Selected features: ['Enterococcus_faecium', 'Lactococcus_lactis', 'Blautia_producta', 'Allisonella_histaminiformans', 'Enterococcus_faecalis', 'Bacteroides_sp_CAG_443', 'Aeriscardovia_aeriphila']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.5343 ± 0.1572\n",
      "  Sensitivity: 0.2000 ± 0.2667\n",
      "  Specificity: 0.8686 ± 0.1304\n",
      "  AUC: 0.6638 ± 0.1150\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "\n",
    "    train_data = data[data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = data[data['Individual_ID'].isin(val_ids)]\n",
    "\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "\n",
    "    extracted_data = extracted_datasets(data, feature_columns)\n",
    "    \n",
    "    extracted_data = plr_transform(extracted_data)\n",
    "\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data)\n",
    "\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "    \n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1)\n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    \n",
    "    minority_class = 1\n",
    "    minority_count = np.sum(y_train == minority_class)\n",
    "    majority_count = np.sum(y_train != minority_class)\n",
    "\n",
    "    new_samples, new_labels = conditional_gmm(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        tp2_train,\n",
    "        minority_class=minority_class,\n",
    "        num_new_samples=num_new_samples,\n",
    "    )\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, new_labels], axis=0)\n",
    "    \n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa6aae3",
   "metadata": {},
   "source": [
    "##### ALR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73131b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Enterococcus_faecium', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Faecalitalea_cylindroides', 'Pseudoflavonifractor_sp_An184']\n",
      "Selected features: ['Coprococcus_eutactus', 'Roseburia_sp_CAG_303', 'Lactococcus_lactis', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Streptococcus_sp_F0442', 'Streptococcus_vestibularis']\n",
      "Selected features: ['Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Bifidobacterium_animalis', 'Butyricimonas_synergistica', 'Sellimonas_intestinalis', 'Faecalitalea_cylindroides', 'Clostridium_perfringens']\n",
      "Selected features: ['Denitrobacterium_detoxificans', 'Enterococcus_faecium', 'Megamonas_funiformis_CAG_377', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Faecalitalea_cylindroides']\n",
      "Selected features: ['Enterococcus_faecium', 'Lactococcus_lactis', 'Blautia_producta', 'Allisonella_histaminiformans', 'Enterococcus_faecalis', 'Bacteroides_sp_CAG_443', 'Aeriscardovia_aeriphila']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.6105 ± 0.1706\n",
      "  Sensitivity: 0.3067 ± 0.3518\n",
      "  Specificity: 0.9143 ± 0.0700\n",
      "  AUC: 0.6738 ± 0.1989\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "\n",
    "    train_data = data[data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = data[data['Individual_ID'].isin(val_ids)]\n",
    "\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "\n",
    "    extracted_data = extracted_datasets(data, feature_columns)\n",
    "    \n",
    "    extracted_data = alr_transform(extracted_data)\n",
    "\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data)\n",
    "\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "    \n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1)\n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    \n",
    "    minority_class = 1\n",
    "    minority_count = np.sum(y_train == minority_class)\n",
    "    majority_count = np.sum(y_train != minority_class)\n",
    "\n",
    "    new_samples, new_labels = conditional_gmm(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        tp2_train,\n",
    "        minority_class=minority_class,\n",
    "        num_new_samples=num_new_samples,\n",
    "    )\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, new_labels], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc08dfde",
   "metadata": {},
   "source": [
    "##### ILR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4a25707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Enterococcus_faecium', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Faecalitalea_cylindroides', 'Pseudoflavonifractor_sp_An184']\n",
      "Selected features: ['Coprococcus_eutactus', 'Roseburia_sp_CAG_303', 'Lactococcus_lactis', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Streptococcus_sp_F0442', 'Streptococcus_vestibularis']\n",
      "Selected features: ['Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Bifidobacterium_animalis', 'Butyricimonas_synergistica', 'Sellimonas_intestinalis', 'Faecalitalea_cylindroides', 'Clostridium_perfringens']\n",
      "Selected features: ['Denitrobacterium_detoxificans', 'Enterococcus_faecium', 'Megamonas_funiformis_CAG_377', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Faecalitalea_cylindroides']\n",
      "Selected features: ['Enterococcus_faecium', 'Lactococcus_lactis', 'Blautia_producta', 'Allisonella_histaminiformans', 'Enterococcus_faecalis', 'Bacteroides_sp_CAG_443', 'Aeriscardovia_aeriphila']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.5124 ± 0.1661\n",
      "  Sensitivity: 0.2133 ± 0.2647\n",
      "  Specificity: 0.8114 ± 0.1227\n",
      "  AUC: 0.6410 ± 0.1996\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "\n",
    "    train_data = data[data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = data[data['Individual_ID'].isin(val_ids)]\n",
    "\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "\n",
    "    extracted_data = extracted_datasets(data, feature_columns)\n",
    "\n",
    "    extracted_data = ilr_transform(extracted_data)\n",
    "\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data)\n",
    "\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "    \n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1)\n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    \n",
    "    minority_class = 1\n",
    "    minority_count = np.sum(y_train == minority_class)\n",
    "    majority_count = np.sum(y_train != minority_class)\n",
    "\n",
    "    new_samples, new_labels = conditional_gmm(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        tp2_train,\n",
    "        minority_class=minority_class,\n",
    "        num_new_samples=num_new_samples,\n",
    "    )\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, new_labels], axis=0)\n",
    "\n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30400fd7",
   "metadata": {},
   "source": [
    "### C: NON-Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28cb29ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['Enterococcus_faecium', 'Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Faecalitalea_cylindroides', 'Pseudoflavonifractor_sp_An184']\n",
      "Selected features: ['Coprococcus_eutactus', 'Roseburia_sp_CAG_303', 'Lactococcus_lactis', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Streptococcus_sp_F0442', 'Streptococcus_vestibularis']\n",
      "Selected features: ['Roseburia_sp_CAG_303', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Bifidobacterium_animalis', 'Butyricimonas_synergistica', 'Sellimonas_intestinalis', 'Faecalitalea_cylindroides', 'Clostridium_perfringens']\n",
      "Selected features: ['Denitrobacterium_detoxificans', 'Enterococcus_faecium', 'Megamonas_funiformis_CAG_377', 'Enterococcus_faecalis', 'Allisonella_histaminiformans', 'Butyricimonas_synergistica', 'Faecalitalea_cylindroides']\n",
      "Selected features: ['Enterococcus_faecium', 'Lactococcus_lactis', 'Blautia_producta', 'Allisonella_histaminiformans', 'Enterococcus_faecalis', 'Bacteroides_sp_CAG_443', 'Aeriscardovia_aeriphila']\n",
      "Average Results:\n",
      "  Balanced_accuracy: 0.5476 ± 0.1510\n",
      "  Sensitivity: 0.2267 ± 0.2939\n",
      "  Specificity: 0.8686 ± 0.0896\n",
      "  AUC: 0.6100 ± 0.1843\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(individual_ids, y_for_fold)):\n",
    "    train_ids = individual_ids[train_idx]\n",
    "    val_ids = individual_ids[val_idx]\n",
    "\n",
    "    train_data = data[data['Individual_ID'].isin(train_ids)]\n",
    "    val_data = data[data['Individual_ID'].isin(val_ids)]\n",
    "\n",
    "    extracted_data_base = train_data.dropna(subset=['Dichotomous_EPDS'])\n",
    "\n",
    "    feature_columns = extract_features(extracted_data_base, k=4)\n",
    "\n",
    "    extracted_data = extracted_datasets(data, feature_columns)    \n",
    "    feature_columns = feature_columns + ['Others']\n",
    "\n",
    "    extracted_data_final, labels = reshape_and_extract_labels(extracted_data)\n",
    "\n",
    "    X = extracted_data_final[:, :2, :].reshape(extracted_data_final.shape[0], -1)\n",
    "    y = labels\n",
    "    \n",
    "    X_train = X[train_idx]\n",
    "    X_val = X[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    \n",
    "    tp2_data = extracted_data_final[:, 2, :].reshape(len(extracted_data_final), -1)\n",
    "    tp2_train = tp2_data[train_idx]\n",
    "    \n",
    "    minority_class = 1\n",
    "    minority_count = np.sum(y_train == minority_class)\n",
    "    majority_count = np.sum(y_train != minority_class)\n",
    "\n",
    "    new_samples, new_labels = conditional_gmm(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        tp2_train,\n",
    "        minority_class=minority_class,\n",
    "        num_new_samples=num_new_samples,\n",
    "    )\n",
    "    \n",
    "    combined_X_train = np.concatenate([X_train, new_samples], axis=0)\n",
    "    combined_y_train = np.concatenate([y_train, new_labels], axis=0)\n",
    "    \n",
    "    for seed in range(num_seeds):\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight=\"balanced\")\n",
    "        rf.fit(combined_X_train, combined_y_train)\n",
    "        \n",
    "        y_pred = rf.predict(X_val)\n",
    "        y_pred_proba = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        metrics = evaluate_model(y_val, y_pred, y_pred_proba)\n",
    "        metrics_list.append(metrics)\n",
    "\n",
    "avg_metrics = {\n",
    "    'Balanced_accuracy': np.mean([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.mean([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.mean([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.mean([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "std_metrics = {\n",
    "    'Balanced_accuracy': np.std([m['Balanced_accuracy'] for m in metrics_list]),\n",
    "    'Sensitivity': np.std([m['Sensitivity'] for m in metrics_list]),\n",
    "    'Specificity': np.std([m['Specificity'] for m in metrics_list]),\n",
    "    'AUC': np.std([m['AUC'] for m in metrics_list]),\n",
    "}\n",
    "\n",
    "print(\"Average Results:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f} ± {std_metrics[metric]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b5c699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c123b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
